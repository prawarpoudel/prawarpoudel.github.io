<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="Prawar Poudel" name="author"/>
  <meta content="" name="keywords"/>
  <meta content="" name="description"/>
  <link href="../css/main.css" rel="stylesheet" type="text/css"/>
  <title>
   Natural Language Processing: Topic Modeling LDA
  </title>
 </head>
 <body>
  <!-- this one is on the top for navigation -->
  <div class="navbar">
   <a href="../index.html">
    Home
   </a>
   <a href="../about.html">
    About
   </a>
   <a href="https://www.linkedin.com/in/prawarpoudel/">
    LinkedIn
   </a>
   <a href="https://scholar.google.com/citations?user=qa8tuSIAAAAJ&amp;hl=en">
    Google Scholar
   </a>
  </div>
  <!-- following is the sidebar -->
  <div class="sidebar">
   <a class="" href="https://prawarpoudel.github.io/about">
    About
   </a>
   <a class="listt" href="../pages/ai_topics.html">
    AI Topics
   </a>
   <a class="listt" href="../pages/datascience_topics.html">
    Data Science Topics
   </a>
   <a class="listt" href="../pages/programming_topics.html">
    Programming Topics
   </a>
   <a class="listt" href="../pages/reading_systemdesign.html">
    System Design Topics (Book)
   </a>
   <a class="listt" href="../pages/kube_topics.html">
    Kubernetes
   </a>
   <a class="listt" href="../pages/coding_topics.html">
    Coding Examples
   </a>
   <a class="listt" href="../pages/ml_basics.html">
    Machine Learning
   </a>
   <a class="listt" href="../pages/rag.html">
    RAG Concept
   </a>
   <a class="listt" href="../pages/rag_code.html">
    RAG Implementation
   </a>
   <a class="listt" href="../pages/redis_benchmarking.html">
    REDIS Benchmarking
   </a>
   <a class="listt" href="../pages/codellama.html">
    Code Llama Intro
   </a>
   <a class="listt" href="../pages/codellama_handson.html">
    Code Llama Hands-On
   </a>
   <a class="listt" href="../pages/breaking_codellama.html">
    Breaking Code Llama
   </a>
   <a class="listt" href="../pages/whyjohnnycantprompt.html">
    Why Jhonny Cant't Prompt
   </a>
   <a class="listt" href="../pages/discriminative.html">
    Discriminative Vs Generative Models
   </a>
   <a class="listt" href="../pages/gan.html">
    Generative Adversarial Models
   </a>
   <a class="listt" href="../pages/regex_py.html">
    Regex in python
   </a>
   <a class="listt" href="../pages/kube_health_checks.html">
    Kube Health Checks
   </a>
   <a class="listt" href="../pages/raft_consensus.html">
    Raft Algorithm (Consensus)
   </a>
   <a class="listt" href="../pages/alerting.html">
    Ex-Google SRE on Alerting
   </a>
   <a class="listt" href="../pages/primer_systemdesign.html">
    System Design Primer (Github)
   </a>
   <a class="listt" href="../pages/supervised_learning.html">
    Supervised Machine Learning
   </a>
   <a class="listt" href="../pages/unsupervised_learning_clustering.html">
    Unsupervised Machine Learning (Clustering)
   </a>
   <a class="listt" href="../pages/unsupervised_learning_dimsreduction.html">
    Unsupervised Machine Learning (Dimensn Redcsn)
   </a>
   <a class="listt" href="../pages/unsupervised_learning_timeseries.html">
    Unsupervised Machine Learning (Timeseries)
   </a>
   <a class="listt" href="../pages/deep_learning.html">
    Deep Learning (Introduction)
   </a>
   <a class="listt" href="../pages/postgresql.html">
    PostgreSQL
   </a>
   <a class="listt" href="../pages/natural_language_processing.html">
    Natural Language Processing
   </a>
  </div>
  <div class="content">
   <h1>
    Natural Language Processing Topics: Topic Modeling LDA
   </h1>
   <p>
    LDA: Latent Dirichlet Allocation
    <ul>
        <li>
            Problem: We have certain number of documents that we want to find topics for. Following are the steps we take:
        </li>
        <li>
            Need to identify K topics before hand.
        </li>
        <li>
            works by going through the document and randomly assigning the words to each of the topics
        </li>
        <li>
            the first pass does not make sense; we iterate over every words in every documents and try to improve the topic.
        </li>
        <li>
            use Bayesian probability formula to compute the metric.
        </li>
    </ul>
   </p>
   <hr>
   <h2>
    Using SKlearn for topic modeling of documents
   </h2>
   <p>
    The dataset used in the following can be found at <a href="https://www.kaggle.com/datasets/gauravduttakiit/npr-data">this link</a>.
   </p>
   <p class="code-text">
    import pandas as pd

    datafile = 'npr.csv'
    df = pd.read_csv(datafile)
    print(df.shape)
    >>>
    (11992, 1)

    print(df.head(2))
    >>>
    Article
    0  In the Washington of 2016, even when the polic...
    1    Donald Trump has used Twitter  —   his prefe...
   </p>
   <p>
    Now let us do count vectorizer
    <p class="code-text">
    from sklearn.feature_extraction.text import CountVectorizer

    # max_df = 0.9, gets rid of words that appear in more than 90% of documents
    # min_df = 2, mean the document must appear in 3 documents
    # the fraction value supplied to max_df and min_df work as the fraction of the no of total documents whereas
    # .. the absolute number means the no of documents (for both max_df and min_df)
    # stop_words to remove the stop words in English
    cv = CountVectorizer(max_df = 0.9, min_df = 2, stop_words='english')
    </p>
    <p class="code-text">
    len(cv.get_feature_names_out())
    >>>
    54777
    </p>
    <p class="code-text">
    dtm = cv.fit_transform(df['Article'])
    </p>
    <p class="code-text">
    from sklearn.decomposition import LatentDirichletAllocation

    # n_components = 7 means we want to model 7 topics
    LDA = LatentDirichletAllocation(n_components=7, random_state=42)
    LDA.fit(dtm)
    </p>
    <p class="code-text">
    # After we do the LDA, we want to
    # 1. grab the vocabulary
    # .. cv.get_feature_names() gives us the words in the documents
    # .. we can grab the words by doing cv.get_feature_names()[9000]
    import random
    random_word_id = random.randint(0,len(cv.get_feature_names_out()))
    random_word = cv.get_feature_names_out()[random_word_id]
    
    # 2. grab the topics
    # .. the topics are saved as LDA.components_.
    # .. LDA.components_ is a matrix  (no of topics=7)X(no of words)
    # .. so we can pick the topic info using LDA.components_[row_index], 
    # .. .. where each values in the column gives the probability of the words to be associated with the topic
    
    # 3. grab the highest probability words per topic
    n = 20
    # let us iterate through each row of the LDA components, i.e. 7 topics
    for i,components in enumerate(LDA.components_):
        print(f"For topic no: {i}, top {n} words are")
        # let us argsort to get the highest probability words from the components, and find the actual words in the cv feature names
        print([cv.get_feature_names_out()[idx] for idx in components.argsort()[(-1*n):]])
        print()
        print()
    </p>
    >>><br>
    For topic no: 0, top 20 words are
['president', 'state', 'tax', 'insurance', 'trump', 'companies', 'money', 'year', 'federal', '000', 'new', 'percent', 'government', 'company', 'million', 'care', 'people', 'health', 'said', 'says']
<br>
<br>

For topic no: 1, top 20 words are
['white', 'according', 'attack', 'reported', 'war', 'military', 'house', 'security', 'russia', 'government', 'npr', 'reports', 'says', 'news', 'people', 'told', 'police', 'president', 'trump', 'said']
<br>
<br>

For topic no: 2, top 20 words are
['little', 'know', 'don', 'year', 'make', 'way', 'world', 'family', 'home', 'day', 'time', 'water', 'city', 'new', 'years', 'food', 'just', 'people', 'like', 'says']
<br>
<br>

For topic no: 3, top 20 words are
['world', 'research', 'university', 'percent', 'care', 'time', 'new', 'don', 'years', 'medical', 'disease', 'patients', 'just', 'children', 'study', 'like', 'women', 'health', 'people', 'says']
<br>
<br>

For topic no: 4, top 20 words are
['donald', 'political', 'states', 'law', 'just', 'voters', 'vote', 'election', 'party', 'new', 'obama', 'court', 'republican', 'campaign', 'people', 'state', 'president', 'clinton', 'said', 'trump']
   </p>
   <p class="code-text">
    #  now for each of the document, let us find the appropriate topic number
    # .. and assign it to them
    
    # .. let us see the probability of each document belonging to the particular topic
    document_probability = LDA.transform(dtm)
    print(document_probability.shape)
    >>>
    (11992, 7)
   </p>
   <p class="code-text">
    # let us create a colum in our original df with a topic column
    df['topic'] = document_probability.argmax(axis=1)
   </p>
   <p>
    And finally,
    <p class="code-text">
    print(df.head(15))
    >>>
    Article  topic
    0   In the Washington of 2016, even when the polic...      1
    1     Donald Trump has used Twitter  —   his prefe...      1
    2     Donald Trump is unabashedly praising Russian...      1
    3   Updated at 2:50 p. m. ET, Russian President Vl...      1
    4   From photography, illustration and video, to d...      2
    5   I did not want to join yoga class. I hated tho...      3
    6   With a   who has publicly supported the debunk...      3
    7   I was standing by the airport exit, debating w...      2
    8   If movies were trying to be more realistic, pe...      3
    9   Eighteen years ago, on New Year’s Eve, David F...      2
    10  For years now, some of the best, wildest, most...      5
    11  For years now, some of the best, wildest, most...      5
    12  The Colorado River is like a giant bank accoun...      2
    13  For the last installment of NPR’s holiday reci...      2
    14  Being overweight can raise your blood pressure...      3
    </p>
   </p>
  </div>
 </body>
</html>
