<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="Prawar Poudel" name="author"/>
  <meta content="Vector Databases, AI, Machine Learning, NLP, GPT" name="keywords"/>
  <meta content="" name="description"/>
  <link href="../css/main.css" rel="stylesheet" type="text/css"/>
  <title>
   Unsupervised Learning
  </title>
 </head>
 <body>
  <!-- this one is on the top for navigation -->
  <div class="navbar">
   <a href="../index.html">
    Home
   </a>
   <a href="../about.html">
    About
   </a>
   <a href="https://www.linkedin.com/in/prawarpoudel/">
    LinkedIn
   </a>
   <a href="https://scholar.google.com/citations?user=qa8tuSIAAAAJ&amp;hl=en">
    Google Scholar
   </a>
  </div>
  <!-- following is the sidebar -->
  <div class="sidebar">
   <a class="" href="https://prawarpoudel.github.io/about">
    About
   </a>
   <a class="listt" href="../pages/ai_topics.html">
    Various AI Topics
   </a>
   <a class="listt" href="../pages/datascience_topics.html">
    Various Data Science Topics
   </a>
   <a class="listt" href="../pages/programming_topics.html">
    Various Programming Topics
   </a>
   <a class="listt" href="../pages/reading_systemdesign.html">
    System Design Topics (Book)
   </a>
   <a class="listt" href="../pages/kube_topics.html">
    Various Kubernetes Topics
   </a>
   <a class="listt" href="../pages/coding_topics.html">
    Various Coding Examples
   </a>
   <a class="listt" href="../pages/ml_basics.html">
    Machine Learning Basics
   </a>
   <a class="listt" href="../pages/rag.html">
    RAG Concept
   </a>
   <a class="listt" href="../pages/rag_code.html">
    RAG Implementation
   </a>
   <a class="listt" href="../pages/redis_benchmarking.html">
    REDIS Benchmarking
   </a>
   <a class="listt" href="../pages/codellama.html">
    Code Llama Intro
   </a>
   <a class="listt" href="../pages/codellama_handson.html">
    Code Llama Hands-On
   </a>
   <a class="listt" href="../pages/breaking_codellama.html">
    Breaking Code Llama
   </a>
   <a class="listt" href="../pages/whyjohnnycantprompt.html">
    Why Jhonny Cant't Prompt
   </a>
   <a class="listt" href="../pages/discriminative.html">
    Discriminative Vs Generative Models
   </a>
   <a class="listt" href="../pages/gan.html">
    Generative Adversarial Models
   </a>
   <a class="listt" href="../pages/regex_py.html">
    Regex in python
   </a>
   <a class="listt" href="../pages/kube_health_checks.html">
    Kube Health Checks
   </a>
   <a class="listt" href="../pages/raft_consensus.html">
    Raft Algorithm (Consensus)
   </a>
   <a class="listt" href="../pages/alerting.html">
    Ex-Google SRE on Alerting
   </a>
   <a class="listt" href="../pages/primer_systemdesign.html">
    System Design Primer (Github)
   </a>
   <a class="listt" href="../pages/supervised_learning.html">
    Supervised Machine Learning
   </a>
   <a class="listt" href="../pages/unsupervised_learning_clustering.html">
    Unsupervised Machine Learning (Clustering)
   </a>
   <a class="listt" href="../pages/unsupervised_learning_dimsreduction.html">
    Unsupervised Machine Learning (Dimensn Redcsn)
   </a>
   <a class="listt" href="../pages/unsupervised_learning_timeseries.html">
    Unsupervised Machine Learning (Timeseries)
   </a>
   <a class="listt" href="../pages/deep_learning.html">
    Deep Learning (Introduction)
   </a>
  </div>
  <div class="content">
   <h1>
    Unsupervised Learning
   </h1>
   <p>
    <ul>
     <li>
      Datapoints do not have any outcomes, or target is unknown.
     </li>
     <li>
      We are interested in the structure of the data or the patterns within the data.
     </li>
     <li>
      Types:
      <ul>
       <li>
        <a href="unsupervised_learning_clustering.html">
         <b>
          Clustering:
         </b>
        </a>
        Algorithm like:
        <ul>
         <li>
          K-Means
         </li>
         <li>
          Hierarchical Agglomerative Clustering
         </li>
         <li>
          DBSCAN
         </li>
         <li>
          Mean shift
         </li>
        </ul>
       </li>
       <li>
        <a href="unsupervised_learning_dimsreduction.html">
         <b>
          Dimensionality Reduction:
         </b>
        </a>
        Algorithm like:
        <ul>
         <li>
          PCA
         </li>
         <li>
          Non-negative matrix factorization
         </li>
         <li>
          They are important because of the
          <mark>
           curse of dimensionality
          </mark>
          .
         </li>
         <li>
          .. which means that as no of features increases performance gets worse, and cost or the number of training examples required increases.
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li>
      Many use cases like:
      <ul>
       <li>
        Classification
       </li>
       <li>
        Anomaly Detection
       </li>
       <li>
        Customer Segmentation
       </li>
       <li>
        Improve Supervised Learning
       </li>
      </ul>
     </li>
    </ul>
   </p>
   <hr/>
   <h2>
    Dimensionality Reduction
   </h2>
   <p>
    <ul>
     One way is
     <b>
      Principal Component Analysis (PCA)
     </b>
     <li>
      Using
      <b>
       Singular Value Decomposition (SVD)
      </b>
     </li>
     <li>
      A feature vector, A, of dimension
      <i>
       mxn
      </i>
      can be decomposed into three metrics using
      <i>
       SVD
      </i>
      .
     </li>
     <li>
      which leads to A
      <i>
       mxn
      </i>
      = U
      <i>
       mxm
      </i>
      * S
      <i>
       mxn
      </i>
      * V transpose
      <i>
       nxn
      </i>
     </li>
     <li>
      S is a diagonal matrix, U and V are square metrices. Principal components are computed from V, i.e. multiple original matrix A with V.Transpose.
     </li>
     <li>
      If we want to reduce data from
      <i>
       mxn
      </i>
      to
      <i>
       mxk
      </i>
      , ie from n features to k features, we single choose U
      <i>
       mxk
      </i>
      , S
      <i>
       kxk
      </i>
      and V.T
      <i>
       kxk
      </i>
      . In this case we multiply, A
      <i>
       mxn
      </i>
      with V.Transpose
      <i>
       nxk
      </i>
     </li>
     <li>
      It is important to scale before PCA, because outliers or distant datasets can skew data.
     </li>
    </ul>
    <p class="code-text">
     from sklearn.decomposition import PCA

PCAins = PCA(n_components=3)
X_trans = PCAins.fit_transform(X_train) #X_trans is our new data
    </p>
   </p>
   <p>
    <ul>
     <b>
      Kernel PCA
     </b>
     <li>
      For non-linear PCA.
     </li>
     <li>
      internally,kernel first maps the data into linear space and applies PCA
     </li>
     <li>
      kernel PCA tend  to preseve the geometric distance between the points
     </li>
     <li>
      <p class="code-text">
       from sklearn.decomposition import KernelPCA

kpca = KernelPCA(n_components=3,Kernel='rbf',gamma=1.0 )

X_trans = kpca.fit_transform(X)
      </p>
     </li>
    </ul>
   </p>
   <p>
    <ul>
     <b>
      Multi-Dimensional Scaling (MDS)
     </b>
     <li>
      for non-linear transformation
     </li>
     <li>
      does not preserve the variance
     </li>
     <li>
      but maintains the geometric distances between points
     </li>
     <li>
      <p class="code-text">
       from sklearn.decomposition import MDS

mds = MDS(n_components=2)
X_trans = mds.fit_transform(X)
      </p>
     </li>
    </ul>
   </p>
   <p>
    Other popular manifold dimensionality reduction methods are Isomap, TSNE.
   </p>
   <p>
    <b>
     Non-Negative Matrix Factorization
    </b>
    <ul>
     <li>
      Same as PCA, but all the matrics must only have positive values
     </li>
     <li>
      for example, for document analysis or pixel values in images
     </li>
     <li>
      powerful for many problems related to documents, texts, images and videos
     </li>
     <li>
      this is powerful also because it can never undo the application of a latent feature since its only addtion
     </li>
     <li>
      more human interpretable
     </li>
     <li>
      since only positive values are considered, it can loose more information when truncating
     </li>
     <li>
      Unlike PCA, it does not give orthogonal latent vectors.
     </li>
     <li>
      Example of document processing:
      <ul>
       <li>
        Input
        <b>
         count vectorizer
        </b>
        or
        <b>
         TF-IDF
        </b>
        processed word document
       </li>
       <li>
        parameters to tune: no of topics, text proprocessing
       </li>
      </ul>
     </li>
     <li>
      <p class="code-text">
       from sklearn.decomposition import NMF

nmf = NMF(n_components=3,init='random')
Xx = nmf.fit(X)
      </p>
     </li>
    </ul>
   </p>
   <hr/>
   <p>
    <a href="unsupervised_learning_clustering.html">
     Clustering at separate page
    </a>
    .
   </p>
  </div>
 </body>
</html>
