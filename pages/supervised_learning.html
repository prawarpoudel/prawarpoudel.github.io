<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="Prawar Poudel" name="author"/>
  <meta content="Vector Databases, AI, Machine Learning, NLP, GPT" name="keywords"/>
  <meta content="" name="description"/>
  <link href="../css/main.css" rel="stylesheet" type="text/css"/>
  <title>
   Supervised Learning
  </title>
 </head>
 <body>
  <!-- this one is on the top for navigation -->
  <div class="navbar">
   <a href="../index.html">
    Home
   </a>
   <a href="../about.html">
    About
   </a>
   <a href="https://www.linkedin.com/in/prawarpoudel/">
    LinkedIn
   </a>
   <a href="https://scholar.google.com/citations?user=qa8tuSIAAAAJ&amp;hl=en">
    Google Scholar
   </a>
  </div>
  <!-- following is the sidebar -->
  <div class="sidebar">
   <a class="" href="https://prawarpoudel.github.io/about">
    About
   </a>
   <a class="listt" href="../pages/ai_topics.html">
    Various AI Topics
   </a>
   <a class="listt" href="../pages/datascience_topics.html">
    Various Data Science Topics
   </a>
   <a class="listt" href="../pages/programming_topics.html">
    Various Programming Topics
   </a>
   <a class="listt" href="../pages/reading_systemdesign.html">
    System Design Topics (Book)
   </a>
   <a class="listt" href="../pages/kube_topics.html">
    Various Kubernetes Topics
   </a>
   <a class="listt" href="../pages/coding_topics.html">
    Various Coding Examples
   </a>
   <a class="listt" href="../pages/ml_basics.html">
    Machine Learning Basics
   </a>
   <a class="listt" href="../pages/rag.html">
    RAG Concept
   </a>
   <a class="listt" href="../pages/rag_code.html">
    RAG Implementation
   </a>
   <a class="listt" href="../pages/redis_benchmarking.html">
    REDIS Benchmarking
   </a>
   <a class="listt" href="../pages/codellama.html">
    Code Llama Intro
   </a>
   <a class="listt" href="../pages/codellama_handson.html">
    Code Llama Hands-On
   </a>
   <a class="listt" href="../pages/breaking_codellama.html">
    Breaking Code Llama
   </a>
   <a class="listt" href="../pages/whyjohnnycantprompt.html">
    Why Jhonny Cant't Prompt
   </a>
   <a class="listt" href="../pages/discriminative.html">
    Discriminative Vs Generative Models
   </a>
   <a class="listt" href="../pages/gan.html">
    Generative Adversarial Models
   </a>
   <a class="listt" href="../pages/regex_py.html">
    Regex in python
   </a>
   <a class="listt" href="../pages/kube_health_checks.html">
    Kube Health Checks
   </a>
   <a class="listt" href="../pages/raft_consensus.html">
    Raft Algorithm (Consensus)
   </a>
   <a class="listt" href="../pages/alerting.html">
    Ex-Google SRE on Alerting
   </a>
   <a class="listt" href="../pages/primer_systemdesign.html">
    System Design Primer (Github)
   </a>
  </div>
  <div class="content">
    <h1>
        Supervised Learning
    </h1>
    <p>
        Data points have known outcome.
    </p>
    <p>
        Goal is to predct the nature of relationship between input parameters and target variables.
    </p>
    <p>
        <b>Parameters:</b> of a machine learning model are 1 or more variables that changes their values as the model learns. No of parameters can 
        range from very few to trillions of parameters.
    </p>
    <p>
        <b>Hyperparameters:</b> are parameters that not learned directly from the data but relates to implementation.
    </p>
    <p>
        Two types of problems:
        <ul>
            <li>
                <b>Regression:</b> y or the outcome variable is numeric.
                <ul>
                    <li>
                        Outcome is continuous.
                    </li>
                </ul>
            </li>
            <li>
                <b>Classification:</b> y or outcome variable is categorical.
                <ul>
                    <li>
                        Outcome is categorical.
                    </li>
                </ul>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        Some terms of interests are:
        <ul>
            <li>
                x: input features
            </li>
            <li>
                yp: Output or the predicted values
            </li>
            <li>
                f(.): prediction function that generates predictions from x and parameters.
            </li>
            <li>
                J(y,yp): Loss function
            </li>
            <li>
                update rule: using features x and outcome y, choose parameters to minimize loss function J
            </li>
        </ul>
    </p>
    <hr>
    <p>
        Interpretation vs Prediction Objective:
        <ul>
            <li>
                <b>Interpretation:</b> Train model to find insights into data. Focus will be on parameters of the model to gain insights; and a 
                less complex models are chosen.
            </li>
            <li>
                <b>Prediction:</b> focus will be on performance metrics of the model. The model cares only about coming with best prediction so can be a black box; 
                complex models can be used. performance metrics involve closeness between yp and y i.e. predicted result vs actual result.
            </li>
        </ul>
    </p>
    <hr>
    <h2>
        Linear Regression
    </h2>
    <p>
        Measures of errors are: They can be used for any regression model
        <ul>
            <li>
                Mean Squared Error (MSE)
            </li>
            <li>
                Sum of Squared Error (SSE)  = sum (error^2)
            </li>
            <li>
                Total Sum of Squares (TSS) = Variance of error
            </li>
            <li>
                Coeffecient of Determination (R^2): 1-(SSE/TSE). Closer to 1 is better.
            </li>
        </ul>
    </p>
    <hr>
    <p>
        It is not a requirement that the target variable is normally distributed; but normally distributed target variable gives better result. <mark>What is required is 
            that the error needs to be normally distributed.
        </mark>
    </p>
    <p>
        If the target variable is not normally distributed, you can make it by transforming it. Then, fit our regression to the transformed values.
    </p>
    <p>
        To see if the target variable is normally distributed, we can see manually or do a statistical test.
        <ul>
            <li>
                For manual approach, we can plot the distribution using <mark>df['target_variable'].hist()</mark>. It should visually show the data 
                thus helping us see if the data is normally distributed.
            </li>
            <li>
                Using <mark>D'Agostino K^2 Test</mark>, you can use library function <mark>normaltest()</mark> from library <mark>scipy.stats.mstats</mark>. This gives out a p-value. 
                The higher p-value indicates the distribution is more closer to being normal. A lower p-value indicates distribution is far low probability of being a normal distribution.
                A threshold of 0.05 or 0.01 can be used for cutoff. 
            </li>
        </ul>
    </p>
    <p>
        To transform a variable (target variable) to make it normally distributed, commonly used techniques are:
        <ul>
            <li>
                <u>Log transform:</u> Just take log of the data. The data will look a lot more normal distributed. This works best for data that exhibits exponential property.
            </li>
            <li>
                <u>Square Root:</u> Just take square root of the data.
            </li>
            <li>
                <u>Box Cox:</u> It is a parameterized transformation.
                <ul>
                    <li>
                        Box-Cox transformed value of a variable y is (y^lambda -1)/lambda.
                    </li>
                    <li>
                        This is a generalization of the square root transformation, but it allows for the root value to vary and find the best one.
                    </li>
                    <li>
                        Use <mark>boxcox from scipy.stats</mark> in code; as <mark>y_transformed = boxcox(df['y'])[0]</mark>. Function <mark>boxcox</mark> returns an array,
                        the first item is the transfored array whereas the second item the lambda that was used.
                    </li>
                </ul>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        How to use:
        <ol>
            <li>
                Import sklearn library as <mark>sklearn.linear_model.LinearRegression</mark>
            </li>
            <li>
                create an object as <mark>LR=LinearRegression()</mark>. You can also pass many other hyperparameters into the object creation.
            </li>
            <li>
                Create X df from the actual dataset by dropping the target variable column so that it is easy for further computation. Similarly,
                create Y by just grabbing the target variable as it column.
            </li>
            <li>
                Fit and transform the X data with the polynomial feature object. 
                <ul>
                    <li>
                        First create an object as <mark>pf = PolynomialFeature(degree=2,include_bias=False)</mark>, that is from <mark>sklearn.preprocessing</mark>. The <mark>include_bias</mark> is False 
                        because later on LinearRegression will take care of that part.
                    </li>
                    <li>
                        Fit and transform as <mark>X_pf = pf.fit_transform(X)</mark>. X_pf now has a lot more columns that what X had.
                    </li>
                </ul>
            </li>
            <li>
                Test-Train split:
                <ul>
                    <li>
                        <mark>x_train, x_test, y_train, y_test = test_train_split(X_pf,y,test_size=0.3,random_state="some int value")</mark> where test_train_split is from 
                        <mark>sklearn.model_selection</mark>.
                    </li>
                </ul>
            </li>
            <li>
                Now apply standard scaler to the train data.
                <ul>
                    <li>
                        s = StandardScalar()
                    </li>
                    <li>
                        x_train_s = s.fit_transform(x_train)
                    </li>
                </ul>
            </li>
            <li>
                Now to bring the target variable to the normal distribution, we will use boxcox as discussed above.
                <ul>
                    <li>
                        <mark>y_train_bc = boxcox(y_train)[0]</mark>
                    </li>
                    <li>
                        We also need the lambda value for later when we need to compute inverse. so, <mark>lam=boxcox(y_train)[1]</mark>
                    </li>
                </ul>
            </li>
            <li>
                Fit the train data as <mark>LR = LR.fit(x_train_s, y_train_bc)</mark>. Here we used:
                <ul>
                    <li>
                        standard scalar fit and transfored x- data.
                    </li>
                    <li>
                        boxcoxed y- data
                    </li>
                </ul>
            </li>
            <li>
                Since the x- data used for modeling was transformed data, let us fit and transform the x_test data to StandardScalar, i.e. <mark>x_test_s = s.transform(x_test)</mark>
            </li>
            <li>
                The predicted value will be boxcoxed transformed; i.e. <mark>y_pred_bc = lr.predict(x_test_s)</mark>.
            </li>
            <li>
                To find the y_pred_bc back to the same scale as y, we can inverse boxcox. <mark>y_pred = inv_boxcox(y_pred_bc,lam)</mark>.
            </li>
            <li>
                Finally, compute the R2 score using <mark>R2 = r2_score(y_pred,y_test)</mark>.
            </li>
            <li>
                Using boxcox on the target variable improves the R2 score (higher is better). In the above example, if done without boxcox, 
                the R2 score will be lower.
            </li>
        </ol>
    </p>
    <h2>
        Data Splits and Cross Validation
    </h2>
  </div>
 </body>
</html>
