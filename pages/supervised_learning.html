<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="Prawar Poudel" name="author"/>
  <meta content="Vector Databases, AI, Machine Learning, NLP, GPT" name="keywords"/>
  <meta content="" name="description"/>
  <link href="../css/main.css" rel="stylesheet" type="text/css"/>
  <title>
   Supervised Learning
  </title>
 </head>
 <body>
  <!-- this one is on the top for navigation -->
  <div class="navbar">
   <a href="../index.html">
    Home
   </a>
   <a href="../about.html">
    About
   </a>
   <a href="https://www.linkedin.com/in/prawarpoudel/">
    LinkedIn
   </a>
   <a href="https://scholar.google.com/citations?user=qa8tuSIAAAAJ&amp;hl=en">
    Google Scholar
   </a>
  </div>
  <!-- following is the sidebar -->
  <div class="sidebar">
   <a class="" href="https://prawarpoudel.github.io/about">
    About
   </a>
   <a class="listt" href="../pages/ai_topics.html">
    Various AI Topics
   </a>
   <a class="listt" href="../pages/datascience_topics.html">
    Various Data Science Topics
   </a>
   <a class="listt" href="../pages/programming_topics.html">
    Various Programming Topics
   </a>
   <a class="listt" href="../pages/reading_systemdesign.html">
    System Design Topics (Book)
   </a>
   <a class="listt" href="../pages/kube_topics.html">
    Various Kubernetes Topics
   </a>
   <a class="listt" href="../pages/coding_topics.html">
    Various Coding Examples
   </a>
   <a class="listt" href="../pages/ml_basics.html">
    Machine Learning Basics
   </a>
   <a class="listt" href="../pages/rag.html">
    RAG Concept
   </a>
   <a class="listt" href="../pages/rag_code.html">
    RAG Implementation
   </a>
   <a class="listt" href="../pages/redis_benchmarking.html">
    REDIS Benchmarking
   </a>
   <a class="listt" href="../pages/codellama.html">
    Code Llama Intro
   </a>
   <a class="listt" href="../pages/codellama_handson.html">
    Code Llama Hands-On
   </a>
   <a class="listt" href="../pages/breaking_codellama.html">
    Breaking Code Llama
   </a>
   <a class="listt" href="../pages/whyjohnnycantprompt.html">
    Why Jhonny Cant't Prompt
   </a>
   <a class="listt" href="../pages/discriminative.html">
    Discriminative Vs Generative Models
   </a>
   <a class="listt" href="../pages/gan.html">
    Generative Adversarial Models
   </a>
   <a class="listt" href="../pages/regex_py.html">
    Regex in python
   </a>
   <a class="listt" href="../pages/kube_health_checks.html">
    Kube Health Checks
   </a>
   <a class="listt" href="../pages/raft_consensus.html">
    Raft Algorithm (Consensus)
   </a>
   <a class="listt" href="../pages/alerting.html">
    Ex-Google SRE on Alerting
   </a>
   <a class="listt" href="../pages/primer_systemdesign.html">
    System Design Primer (Github)
   </a>
  </div>
  <div class="content">
    <h1>
        Supervised Learning
    </h1>
    <p>
        Data points have known outcome.
    </p>
    <p>
        Goal is to predct the nature of relationship between input parameters and target variables.
    </p>
    <p>
        <b>Parameters:</b> of a machine learning model are 1 or more variables that changes their values as the model learns. No of parameters can 
        range from very few to trillions of parameters.
    </p>
    <p>
        <b>Hyperparameters:</b> are parameters that not learned directly from the data but relates to implementation.
    </p>
    <p>
        Two types of problems:
        <ul>
            <li>
                <b>Regression:</b> y or the outcome variable is numeric.
                <ul>
                    <li>
                        Outcome is continuous.
                    </li>
                </ul>
            </li>
            <li>
                <b>Classification:</b> y or outcome variable is categorical.
                <ul>
                    <li>
                        Outcome is categorical.
                    </li>
                </ul>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        Some terms of interests are:
        <ul>
            <li>
                x: input features
            </li>
            <li>
                yp: Output or the predicted values
            </li>
            <li>
                f(.): prediction function that generates predictions from x and parameters.
            </li>
            <li>
                J(y,yp): Loss function
            </li>
            <li>
                update rule: using features x and outcome y, choose parameters to minimize loss function J
            </li>
        </ul>
    </p>
    <hr>
    <p>
        Interpretation vs Prediction Objective:
        <ul>
            <li>
                <b>Interpretation:</b> Train model to find insights into data. Focus will be on parameters of the model to gain insights; and a 
                less complex models are chosen.
            </li>
            <li>
                <b>Prediction:</b> focus will be on performance metrics of the model. The model cares only about coming with best prediction so can be a black box; 
                complex models can be used. performance metrics involve closeness between yp and y i.e. predicted result vs actual result.
            </li>
        </ul>
    </p>
    <hr>
    <h2>
        Linear Regression
    </h2>
    <p>
        Measures of errors are: They can be used for any regression model
        <ul>
            <li>
                Mean Squared Error (MSE)
            </li>
            <li>
                Sum of Squared Error (SSE)  = sum (error^2)
            </li>
            <li>
                Total Sum of Squares (TSS) = Variance of error
            </li>
            <li>
                Coeffecient of Determination (R^2): 1-(SSE/TSE). Closer to 1 is better.
            </li>
        </ul>
    </p>
    <hr>
    <p>
        It is not a requirement that the target variable is normally distributed; but normally distributed target variable gives better result. <mark>What is required is 
            that the error needs to be normally distributed.
        </mark>
    </p>
    <p>
        If the target variable is not normally distributed, you can make it by transforming it. Then, fit our regression to the transformed values.
    </p>
    <p>
        To see if the target variable is normally distributed, we can see manually or do a statistical test.
        <ul>
            <li>
                For manual approach, we can plot the distribution using <mark>df['target_variable'].hist()</mark>. It should visually show the data 
                thus helping us see if the data is normally distributed.
            </li>
            <li>
                Using <mark>D'Agostino K^2 Test</mark>, you can use library function <mark>normaltest()</mark> from library <mark>scipy.stats.mstats</mark>. This gives out a p-value. 
                The higher p-value indicates the distribution is more closer to being normal. A lower p-value indicates distribution is far low probability of being a normal distribution.
                A threshold of 0.05 or 0.01 can be used for cutoff. 
            </li>
        </ul>
    </p>
    <p>
        To transform a variable (target variable) to make it normally distributed, commonly used techniques are:
        <ul>
            <li>
                <u>Log transform:</u> Just take log of the data. The data will look a lot more normal distributed. This works best for data that exhibits exponential property.
            </li>
            <li>
                <u>Square Root:</u> Just take square root of the data.
            </li>
            <li>
                <u>Box Cox:</u> It is a parameterized transformation.
                <ul>
                    <li>
                        Box-Cox transformed value of a variable y is (y^lambda -1)/lambda.
                    </li>
                    <li>
                        This is a generalization of the square root transformation, but it allows for the root value to vary and find the best one.
                    </li>
                    <li>
                        Use <mark>boxcox from scipy.stats</mark> in code; as <mark>y_transformed = boxcox(df['y'])[0]</mark>. Function <mark>boxcox</mark> returns an array,
                        the first item is the transfored array whereas the second item the lambda that was used.
                    </li>
                </ul>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        How to use:
        <ol>
            <li>
                Import sklearn library as <mark>sklearn.linear_model.LinearRegression</mark>
            </li>
            <li>
                create an object as <mark>LR=LinearRegression()</mark>. You can also pass many other hyperparameters into the object creation.
            </li>
            <li>
                Create X df from the actual dataset by dropping the target variable column so that it is easy for further computation. Similarly,
                create Y by just grabbing the target variable as it column.
            </li>
            <li>
                Fit and transform the X data with the polynomial feature object. 
                <ul>
                    <li>
                        First create an object as <mark>pf = PolynomialFeature(degree=2,include_bias=False)</mark>, that is from <mark>sklearn.preprocessing</mark>. The <mark>include_bias</mark> is False 
                        because later on LinearRegression will take care of that part.
                    </li>
                    <li>
                        Fit and transform as <mark>X_pf = pf.fit_transform(X)</mark>. X_pf now has a lot more columns that what X had.
                    </li>
                </ul>
            </li>
            <li>
                Test-Train split:
                <ul>
                    <li>
                        <mark>x_train, x_test, y_train, y_test = test_train_split(X_pf,y,test_size=0.3,random_state="some int value")</mark> where test_train_split is from 
                        <mark>sklearn.model_selection</mark>.
                    </li>
                </ul>
            </li>
            <li>
                Now apply standard scaler to the train data.
                <ul>
                    <li>
                        s = StandardScalar()
                    </li>
                    <li>
                        x_train_s = s.fit_transform(x_train)
                    </li>
                </ul>
            </li>
            <li>
                Now to bring the target variable to the normal distribution, we will use boxcox as discussed above.
                <ul>
                    <li>
                        <mark>y_train_bc = boxcox(y_train)[0]</mark>
                    </li>
                    <li>
                        We also need the lambda value for later when we need to compute inverse. so, <mark>lam=boxcox(y_train)[1]</mark>
                    </li>
                </ul>
            </li>
            <li>
                Fit the train data as <mark>LR = LR.fit(x_train_s, y_train_bc)</mark>. Here we used:
                <ul>
                    <li>
                        standard scalar fit and transfored x- data.
                    </li>
                    <li>
                        boxcoxed y- data
                    </li>
                </ul>
            </li>
            <li>
                Since the x- data used for modeling was transformed data, let us fit and transform the x_test data to StandardScalar, i.e. <mark>x_test_s = s.transform(x_test)</mark>
            </li>
            <li>
                The predicted value will be boxcoxed transformed; i.e. <mark>y_pred_bc = lr.predict(x_test_s)</mark>.
            </li>
            <li>
                To find the y_pred_bc back to the same scale as y, we can inverse boxcox. <mark>y_pred = inv_boxcox(y_pred_bc,lam)</mark>.
            </li>
            <li>
                Finally, compute the R2 score using <mark>R2 = r2_score(y_pred,y_test)</mark>.
            </li>
            <li>
                Using boxcox on the target variable improves the R2 score (higher is better). In the above example, if done without boxcox, 
                the R2 score will be lower.
            </li>
        </ol>
    </p>
    <h2>
        Data Splits and Cross Validation
    </h2>
    <p>
        To split to have a hold out data, so that its used for cross validation.
    </p>
    <p>
        Training data: 
        <ul>
            <li>
                Used for training.
            </li>
            <li>
                <mark>model(x_train,y_train).fit()</mark> = model
            </li>
        </ul>
    </p>
    <p>
        Test data:
        <ul>
            <li>
                Used for testing and prediction.
            </li>
            <li>
                <mark>model.predict(x_test)</mark> = y_pred
            </li>
            <l>
                Use actual y_test to compare and calculate <mark>test_error</mark>.
            </li>
        </ul>
    </p>
    <p>
        Syntax for test train split
        <ul>
            <li>
                <mark>from sklearn.model_selection import test_train_split</mark>
            </li>
            <li>
                <mark>train,test = test_train_split(data,test_size=0.3)</mark>
            </li>
            <li>
                <mark>x_train,x_test, y_train, y_test = test_train_split(x,y,test_size=0.3)</mark>
            </li>
            <li>
                There are other many ways for splitting, like shuffle split or stratified shuffle split. <br>
                <mark>from sklearn.model_selection import ShuffleSplit</mark>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        Categorical Data is one-hot encoded. The number of columns that is made as a result of one-hot encoding is equal to no of category values 
        - 1. The process is outline below:
        <ul>
            <li>
                Find the columns whose dtypes is np.object, i.e. <mark>mask = df.dtypes==np.object</mark>.
            </li>
            <li>
                And, filter out the columns <mark>cols = df.columns[mask]</mark>. These are the columns that we want to apply one-hot encoding to.
            </li>
            <li>
                We would also like to see if the no of unique values in the columns are more than one. If there is only 1 unique value, it does not
                make sense to one-hot encode.
            </li>
        </ul>
    </p>
    <p>
        Using <mark>sklearn.preprocessing.OneHotEncoder</mark>, instead of <mark>pd.get_dummies</mark>
        <ul>
            <li>
                <mark>df_copy = df.copy()</mark>
            </li>
            <li>
                instantiate one hot encoder object, <mark>ohc = OneHotEncoder()</mark>
            </li>
            <li>
                inside a for loop for each column, <b>col</b> in the <mark>cols</mark> list:
                <ul>
                    <li>
                        <mark>dat = ohc.fit_transform(df_copy[col])</mark>
                    </li>
                    <li>
                        drop the original column, <mark>df_copy.drop(col,axis=1)</mark>
                    </li>
                    <li>
                        get the name of all the new columns, <mark>new_cols = ohc.categories_</mark>
                    </li>
                    <li>
                        create name of columns so that its easy to join in the df later <br>
                        <mark>new_cols = ['_'.join([col,catt]) for cat in new_cols[0]]</mark>
                    </li>
                    <li>
                        create a df from the one hot encoded data <br>
                        <mark>new_df =  pd.DataFrame(dat.toarray(),columns=new_cols)</mark>
                    </li>
                    <li>
                        now append to our copy df <br>
                        <mark>df_copy = pd.concat([df_copy,new_df],axis=1 )</mark>
                    </li>
                </ul>
            </li>
            <li>
                One potential issue with too many one-hot encoded df is that Too many parameters, overfitting of model. Error will be high in test data is overfit.
            </li>
        </ul>
    </p>
    <hr>
    <p>
        Applying scalar like <mark>StandardScalar</mark> or <mark>MinMaxScalar</mark>, make sure to call <mark>fit_transform()</mark> on the training data;
        but only <mark>transform()</mark> on the test data. This should be done before applying linearregression. An example is given below.
        <ul>
            <li>
                create an object, for example <br>
                <mark>s = StandardScalar()</mark>
            </li>
            <li>
                fit transform x_train data <br>
                x_train_s = s.fit_transform(x_train)
            </li>
            <li>
                transform x_test data <br>
                x_test_s = s.transform(x_test)
            </li>
            <li>
                fit the linearRegression model <br>
                LR.fit(x_train_s,y_train)
            </li>
            <li>
                predict <br>
                predictions = LR.predict(x_test_s)
            </li>
            <li>
                find error <br>
                error = mean_squared_error(y_test,predictions)
            </li>
        </ul>
    </p>
    <h3>
        Cross Validation
    </h3>
    <p>
        Here we use multitple validation sets. These are test sets that are disjoint of each other. For each case of validation sets, the train dataset 
        can be a subset of the remaining dataset. There should be no overlap between the validation or test splits in each iteration of experiment. Training data 
        can have overlap.
    </p>
    <p>
        The average error across all these validation sets is the <mark>cross validation result</mark>.
    </p>
    <p>
        As the model gets more complex, the train error will minimize. But with the cross validation, there is an inflection point, and as the complexity
        increases, the error will increase. This is because too complex model will not generalize properly, and overfit. This we should stop increasing the complexity
        as soon as the crossvalidation error starts to increase.
        <img src="../images/crossvalidation.png" class="center-image">
    </p>
    <p>
        Coding example:
        <ul>
            <li>
                import library <br>
                <mark>from sklearn.model_selection import cross_val_score</mark>
            </li>
            <li>
                perform cross val score <br>
                <mark>cross_val = cross_val_score(model, x_data, y_data, cv=4, scoring='neg_mean_squared_error')</mark>
            </li>
            <li>
                <b>Other methods as follows are also available</b>
                <mark>from sklearn.model_selection import KFold, StratifiedKFold </mark>
            </li>
        </ul>
    </p>
    <h3>
        Using Pipeline and Crossvalidation
    </h3>
    <p>
        <ul>
            <li>
                Import all needed library
                <mark>from sklearn.linear_model import LinearRegression, Lasso, Ridge</mark>
            </li>
            <li>
                From the dataset, acquire the target variable as Y, and drop the value from original df to save it as X.
            </li>
            <li>
                Use kfold that is imported as <mark><br>from sklearn.model_selection import KFold, cross_val_predict</mark>
                <mark>
                    <br>kf = KFold(shuffle=True, random_state=6644, n_splits=3)
                </mark>
                <br>This means we will have 3 training and 3 test sets. Training sets may overlap, but test sets will not overlap.
            </li>
            <li>
                kf.slit(X) will give a generator object that has indices. For example, iterating through the generator gives a tuple with x-indices and y-indices as follows:
                <mark>
                    <br>for train_index, test_index in kf.split(X):
                </mark>
                <br>The index values, <mark>train_index, test_index</mark> can be anything from 0 to length of X-1, with length being determined by the split value.
            </li>
            <li>
                Once the indices are obtained, extract the value from the X and Y using the indices.
                <mark>
                    <br>for train_index, test_index in kf.split(X): <br>
                        ..X_train,X_test, y_train, y_test = (X.iloc[train_index], X.iloc[test_index],y[train_index],y[test_index]) <br>
                        ..lr.fit(X_train, y_train) <br>
                        ..y_pred = lr.predict(X_test) <br>
                        ..score = r2_score(y_test.values, y_pred) <br>
                </mark>
                <br> This will show the scores for all the splits.
            </li>
            <li>
                Let us add standard scalar into this. <b>Without any regularization, scaling does not help Linear Regression.</b> But just to see an example, following can be done.
                <mark>
                    <br>s = StandardScalar()
                    <br>for train_index, test_index in kf.split(X): <br>
                        ..X_train,X_test, y_train, y_test = (X.iloc[train_index], X.iloc[test_index],y[train_index],y[test_index]) <br>
                        ..X_train_s = s.fit_transform(X_train) <br>
                        ..lr.fit(X_train_s, y_train) <br>
                        ..X_test_s = s.transform(X_test)
                        ..y_pred = lr.predict(X_test_s) <br>
                        ..score = r2_score(y_test.values, y_pred) <br>
                </mark>
                <br> This will show the scores for all the splits.
            </li>
        </ul>
    </p>
    <hr>
    <p>
        Using pipeline
        <ul>
            <li>
                Sklearn allows to chain multiple operator items, as long as as they have fit() method.
            </li>
            <li>
                Here, output of one is input of another. So they also need to have fit_transform() method.
            </li>
            <li>
                The above code will now be following using pipeline:
                <ul>
                    <li>
                        <mark>s = StandardScalar()</mark> <br>
                        <mark>lr = LinearRegression()</mark>
                    </li>
                    <li>
                        <mark>my_pipe = Pipeline([('scalar',s),('regression',lr)])</mark>
                    </li>
                    <li>
                        <mark>kf = KFold(shuffle=True, random_state=6644, n_splits=3)</mark>
                    </li>
                    <li>
                        <mark>predictions = cross_val_predict(my_pipe,X,y,cv=kf)</mark>
                    </li>
                    <li>
                        <mark>r2_score(y,predictions</mark>
                    </li>
                </ul>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        <u>HyperParameter Tuning, Lasso Regression, PolynomialFeature</u>
        <ul>
            <li>
                function used is <mark>np.geomspace</mark>
            </li>
            <li>
                For <mark>Lasso Regression</mark>, higher value of alpha means the model is less complex whereas lower value of alpha means model is more complex.
            </li>
            <li>
                Less value of alpha for lasso makes the model similar to LinearRegression.
            </li>
            <li>
                Lasso is iniitalized as <br>
                <mark>lasso=Lasso(alpha=alpha, max_iter=10000)</mark>
            </li>
            <li>
                Complete usage with different values of alpha for hyperparameters selection is as follows: <br>
                <mark>
                    alphas = np.geomspace(1e-9,1e10,num=10) #creates equally spaced 10 numbers, geometrically spaced <br>
                    for alpha in alphas: <br>
                    .. lasso = Lasso(alpha=alpha,max_iter=100000) <br>
                    .. my_pipe = Pipeline([('scalar':s),("lasso":lasso)]) <br>
                    .. pred = cross_val_predict(my_pipe,X,y,cv=kf) <br>
                    .. score = r2_score(y,pred)
                </mark>
            </li>
            <li>
                With Lasso, its is always better to scale the data before using lasso regression.
            </li>
        </ul>
    </p>
    <p>
        You can add <mark>PolynomialFeature</mark> to the pipeline. What a PolynomialFeature does is very well explained 
        at <a href="https://machinelearningmastery.com/polynomial-features-transforms-for-machine-learning/">this link</a>. In a nutshell, it raises the input 
        variables to a polynomial degree. So, after applying the PolynomialFeature, the input dimension now increases.
    </p>
    <p>The above example thus becomes:
        <ul>
            <li>
                <mark>
                    pf = PolynomialFeature(degree=3)<br>
                    alphas = np.geomspace(1e-9,1e10,num=10) #creates equally spaced 10 numbers, geometrically spaced <br>
                    for alpha in alphas: <br>
                    .. lasso = Lasso(alpha=alpha,max_iter=100000) <br>
                    .. my_pipe = Pipeline([('polyfeat':pf),('scalar':s),("lasso":lasso)]) <br>
                    .. pred = cross_val_predict(my_pipe,X,y,cv=kf) <br>
                    .. score = r2_score(y,pred)
                </mark>
            </li>
        </ul>
    </p>
    <p>
        After going through the different scores based on various <mark>alpha</mark> values, we can find the best among all. And finally train the model as follows:
        <ul>
            <li>
                <mark>
                    best_pipe = Pipeline([ <br>
                        ("poly_feat":PolynomialFeature(degree=2)), <br>
                        ("scalar":s), <br>
                        ("lasso":Lasso(alpha=0.01,max_iter=100000)) <br>
                    ]) <br>
                    best_pipe.fit(X,y) <br>
                    best_pipe.score(X,y) <br>
                </mark>
            </li>
            <li>
                To see the coeffecients, we can <br>
                <mark>
                    best_pipe.named_steps['Lasso'].coef_
                </mark>
            </li>
            <li>
                Ridge Regression also works the same way as far as the coding aspect is concerned.
            </li>
            <li>
                <p class="important">
                    From the pipeline estimator that we created, we can actually see the interaction of the different input variables and the their contribution to the output variable.
                </p>
                <p>
                    From the pipeline, polynomial feature gives the higher power of the input variable as well as the interaction components. The corresponding coefficient of the Lasso/Ridge
                    regression model will indicate their relative contribution. Higher positive value indicates positive impact whereas higher negative value indicates negative impact.
                </p>
                <p>
                    <mark>from the best_pipe.named_steps['poly_feat'].get_feature_names(input_features=X.columns)</mark> gives the feature names whereas as seen before <mark>.coef_</mark> gives the lasso/ridge coefficient.
                </p>
            </li>
        </ul>
    </p>
    <p class="important">
        Stratified cross validation is not equivalent to k-fold cross validation with k=N-1 where N is no of features.
    </p>
    <p class="important">
        For a linear regression model, stratified cross-validation with same k will not increase the variance of estimated parameters, as compared to k-fold cross validation.
    </p>
    <p class="important">
        For k-fold cross validation, variance of the estimated model parameters will increase across subsamples with increase in k.
    </p>
    <hr>
    <p>
        <u>Grid Search CV</u>
        <ul>
            <li>
                define pipeline as follows <br>
                <mark>
                    estimator = Pipeline ([ <br>
                    .. ('polynomial_features', PolynomialFeatures()), <br>
                    .. ('scalar',StandardScalar()), <br>
                    .. ('ridge_regression',Ridge()) <br>
                    .. ])
                </mark>
            </li>
            <li>
                create <mark>parameters</mark> as following: <br>
                <mark>
                    params = { <br>
                        'polynomial_features__degree' : [1,2,3], <br>
                        'ridge_regression__alpha' : np.geomspace(4,20,30) <br>
                    } <br>
                </mark>
                The name of the items keys in parameters dict comes from <mark>PipeLine component name</mark> + two underscores + <mark>its property</mark>
            </li>
            <li>
                create a <mark>GridSearchCV()</mark> object as: <br>
                <mark>
                    grid = GridSearchCV(estimator, params,cv=kf)
                </mark>
            </li>
            <li>
                Fit the data: <br>
                <mark>
                    grid.fit()
                </mark>
            </li>
            <li>
                Predict as <br>
                <mark>
                    y_predict = grid.predict(X)
                </mark>
            </li>
            <li>
                Print the best metrics as <br>
                <mark>grid.best_score_, grid.best_params_</mark>
            </li>
            <li>
                If we want to look at the coeff of the estimators, we can do following <br>
                <mark>grid.best_estimator_.named_steps['ridge_regression'].coeff_</mark>
            </li>
            <li>
                To see the scores for all the variables that it searched through, and the scores obtained for each of those variables <br>
                <mark>
                    pd.DataFrame(grid.cv_results_)
                </mark>
            </li>
        </ul>
    </p>
    <h3>
        polynomial Regression
    </h3>
    <p> Following are some of the approaches for dealing with fundamental problems of <b>prediction</b> and <b>interpretation</b>: 
        <ul>
            <li>
                Extending linear regression
            </li>
            <li>
                using polynomial features to capture non-linear effects.
            </li>
        </ul>
    </p>
    <p>
        We want to capture higher order features of data by adding polynomial features. The relation may still be linear, but with a higher degree terms.
    </p>
    <p>
        This can also include variable interactions,like x1x2 in addition to x1^2 and x2^2 from x1 and x2.
    </p>
    <p>
        Using polynomial features example as somewhere above, we do following:
        <ul>
            <li>create polyFit with degree of <b>m</b></li>
            <li>fit the X data</li>
            <li>transform the X data</li>
        </ul>
        This will create the interaction terms also.
    </p>
    <hr>
    <h3>
        Bias and Variance
    </h3>
    <p>
        <b>Bias</b>: is a tendency to miss. They are guided by similar error, and are generally with low variance  and the data points are consistently errored.
    </p>
    <p>
        <b>Variance</b>: is a tendency to be inconsistent. It can be thought of as a metric to refer to model's sensitivity.
    </p>
    <p>
        <img class="center-image" src="../images/biasvariance.png">
    </p>
    <p>
        Above image shows an example for definition of Bias and Variance in terms of shooting to an target. Ideally, we would want our
        model to have an outcome that is <mark>top-left outcome</mark>, with small bias and the preditions (or shootings) very close to the target.
    </p>
    <p>
        <b>Three Source of Errors in a Model</b>:
        <ul>
            <li>
                Model being wrong: Bias
                <ul>
                    <li>
                        Can be because of missing information
                    </li>
                    <li>
                        Can be because of simple model
                    </li>
                    <li>
                        Miss real pattern, because model is underfit.
                    </li>
                </ul>
            </li>
            <li>
                Model being unstable: Variance
                <ul>
                    <li>
                        Characterized by high changes in the output because of small changes in the input.
                    </li>
                    <li>
                        Overly complex or poorly fit models
                    </li>
                    <li>
                        Overfitting of model
                    </li>
                </ul>
            </li>
            <li>
                Unavoidable Randomness: Random error that cannot be reduced
            </li>
        </ul>
    </p>
    <p>
        <img src="../images/tradeoffs.png" class="center-image">
    </p>
    <p>
        For polynomial regression, higher degree means model is complex. Thus high variance and lowered bias.
    </p>
    <hr>
    <h3>Regularization and Model Selection</h3>
    <p>
        Regularization is an approach to handle over-fitting.
    </p>
    <p>
        Add a new cost function to the old cost function, with a <mark>Regularization Strength Parameter</mark>. Thus a new cost adjusted cost 
        function is created as <mark>M(w) + lambda * R(w)</mark>, where M(w) is the model error, R(w) is the function of estimated paramters and 
        <mark>lambda</mark> is the strength parameter.
    </p>
    <p>
        The adjustable regularization strength paramter can be used to penalize the model if it is too complex. This can be 
        used to dumb down the model. <mark>lambda</mark> adds a penalty propoortional to the size of the estimated model parameter.
    </p>
    <p>
        Regularization strength parameter allows us to manage complexity tradeoffs:
        <ul>
            <li>
                more regularization, less complex model or more bias
            </li>
            <li>
                less regularization, more complex model or increased variance
            </li>
        </ul>
    </p>
    <p>
        <ul>
            <li>
                <b>
                    L1 Regularization:
                </b> Drives some coeffecients dowm to zero,  because the added model parameters values are single degree multiplied by the lambda.
                <p class="important">
                    L1 regularization imposes Laplacian prior.
                </p>
            </li>
            <li>
                <b>
                    L2 Regularization:
                </b> On the other hand, adds squared model parameters to the cost function.
                <p class="important">
                    L2 regularization imposes Gaussian prior.
                </p>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        <b>
            Ridge Regression
        </b>
        <ul>
            <li>
                Ridge Regression is a L2 regularized linear regression.
            </li>
            <li>
                There is influence of standardization on Ridge regression.
            </li>
            <li>
                Regularization has shrinking effect, some coeffecients towards 0 due to the penalty. The added term is the 
                difference between the Linear Regressio and Ridge Regression. It uses L2 regularization on linear regression.
            </li>
            <li>
                There is squaring, so larger weights are more penalized.
            </li>
            <li>
                As regularization strenght increases for ridge regression, the shinkage effect is seen, i.e., we will decrease each coeffecients.
            </li>
            <li>
                The reduction of variance may outpace the increase in bias, which leads to better model.
            </li>
            <li>
                With ridge regression, we have <mark>RidgeCV</mark>, that also perform cross validation at the same time.
                <p>
                    <mark>
                        from sklearn.linear_model import RidgeCV <br>
                        alphas = [0.005, 0.01, 0.05, 0.1] <br>
                        ridcv = RidgeCV(alpha=alphas,cv=4).fit(x_train, y_train) <br>
                        v_pred = ridcv.predict(x_test)
                    </mark>
                </p>
                The best alpha value can be extracted as <mark>ridcv.aplha_</mark>, and the coeffecients of the model can be
                listed as a list <mark>ridcv.coef_</mark>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        <b>
            Lasso Regression
        </b>
        <ul>
            <li>
                Lasso is L1 regularized Linear Regression. LASSO stands for <mark>Least Absolute Shrinkage and Selection Operator</mark>
            </li>
            <li>
                The complexity penalty lambda is proportional to the absolute value of the coeffecients.
            </li>
            <li>
                More likely to perform <mark>feature selection</mark>, because it will zero out some of the terms as it uses L1 regularization.
            </li>
            <li>
                Lasso have higher interpretability than Ridge. But is slower to converge than Ridge so the timing may be high for Lasso.
            </li>
            <li>
                May also underperform if the target truly depends on many of the features.
            </li>
            <li>
                We also have a lassocv similar to Ridge cv discussed above. An additional term to provide is <mark>nax_iter</mark> because lasso does not 
                converge as fast.
            </li>
        </ul>
    </p>
    <hr>
    <p>
        <b>
            Elastic Net
        </b>
        <ul>
            <li>
                A hybrid approach that introduces a new parameter alpha that determines a weighted average of L1 and L2 penalties.
            </li>
            <li>
                ElasticNetCV is also a class similar to LassoCV and RidgeCN, but with an additional parameter called <mark>l1_ratio</mark>
                in addition to l1_ratio and alphas.
            </li>
        </ul>
    </p>
    <hr>
    <p>
        <b>
            Recursive Feature Elimination (RFE): RFE is an approach that combines
        </b>
        <ul>
            <li>
                a model or estimation approach;
            </li>
            <li>
                a desired number of features
            </li>
        </ul>
    </p>
    <p>
        RFE repeatedly applies the model, measures feature importance and recursively removes the less important features.
    </p>
    <p>
        RFE example is as follows: <br>
        <mark>
            from sklearn.feature_selection import RFE <br>
            rfe = RFE(est, n_features_to_select=5) <br>
            rfe = rfe.fit(X_train, y_train) <br>
            y_pred = rfe.predict(X_test)
        </mark>
    </p>
    <p>
        A class <mark>RFECV</mark>, uses RFE with cross validation.
    </p>
    <p>
        <ul>
            <li>
                regularization forces the range of coefficients to be smaller, restricted. Smaller range of 
                coeffecients will have lower variance.
            </li>
        </ul>
    </p>
    <hr>
    <p>
        So overall steps as of now for <mark>LinearRegression</mark> are:
        <ul>
            <li>
                Isolate target variable as y. <br>
                <mark>y = df[y_col]</mark>
            </li>
            <li>
                Create other cols as X <br>
                <mark>
                    X = df.drop[y_col,axis=1]
                </mark>
            </li>
            <li>
                test train split
            </li>
            <li>
                Apply standardization to the data. Most common is to use <mark>StandardScalar</mark>. <br>
                <mark>
                    s = StandardScalar() <br>
                    X_s = S.fit(X)
                </mark>
            </li>
            <li>
                Apply LinearRegression <br>
                <mark>
                    lr = LinearRegression() <br>
                    lr = lr.fit(X_s, y) <br>
                </mark>
            </li>
            <li>
                See the coeffecients <br>
                <mark>
                    print(lr.coef_)
                </mark>
            </li>
            <li>
                To see what input vairable has what coeff, we can do following: <br>
                <mark>
                    pd.DataFrame(zip(X.columns,lr2.coef_)).sort_values(by=1)
                </mark>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        For Lasso Regression, we introduce the <mark>PolynomialFeatures</mark> with certain degree also.
        <ul>
            <li>
                First, fit_transform() with PolynomialFeature
            </li>
            <li>
                test train split
            </li>
            <li>
                Then, fit_transform(x_train) with StandardScalar
            </li>
            <li>
                Then, lasso().fit(x_train,y)
            </li>
            <li>
                Then lasso().predict(StandardScalar.transform(x_test))
            </li>
            <li>
                Then, compute r2_score or something
            </li>
        </ul>
    </p>
    <hr>
    <h2>
        Logistic Regression
    </h2>
    <p>
        Used for classification. An extension of Linear Regression but handles what Linear Regression gets wrong.
    
    </p>
    <p>
        <b>Sigmoid Function</b> <br>
        <img src="../images/sigmoid.png" class="center-image">
    </p>
    <p>
        The use of sigmoid can be useful for classification in some of the obvious separable classes (for example), as below image prsents. <br>
        <img src="../images/sigmoid_classification.png"> <br>
        We can see how it can classify between two classes with a clear decision boundary.
    </p>
    <p>
        <mark>
            import sklearn.linear_model import LogisticRegression <br>
            lr = LogisticRegression(penalty='l2', c=10.0) <br>
            lr = lr.fit(x_train, y_train) <br>
            y_predict = lr.predict(x_test) <br>
            lr.coef_
        </mark>
        <br>
        Above, <b>c</b> is the regularization constant (here higher c means less lambda, so is inverse).
    </p>
    <p class="important">
        LogisticRegression also comes with LogisticRegressionCV.
    </p>
    <hr>
    <p>
        <b>Confusion Matrix</b>: <br>
        <img src="../images/confusion.png" class="center-image">
        <ul>
            <li>
                False Positive: Type-I error
            </li>
            <li>
                False Negative: Type-II error
            </li>
            <li>
                Accuracy = (True Positive + True Negative)/All Observations
            </li>
            <li>
                Recall/Sensitivity = (True Positive)/(True Positive + False Negative) <br>
                <mark>But a major flaw here is a model can predict all as True Positive, and have a 100% Recall.</mark>
            </li>
            <li>
                Precision = (True Positive)/(True Positive + False Positive) <br>
                <mark>Precision helps to balance the flaw of Recall. It shows how often, from among the positive predicted values, it gets the prediction correct.</mark>
            </li>
            <li>
                Specificity = (True Negative)/(False Positive + True Negative) <br>
                <mark>Specificity is the recall for negative class.</mark>
            </li>
            <li>
                F1 Score = Harmonic Mean of Precision and Recall = 2* (Precision * Recall)/(Precision + Recall)
            </li>
        </ul>
    </p>
    <hr>
    <p>
        <b>ROC Curve: Receiver Operating Characteristic</b> <br>
        <img src="../images/roc.png" class="center-image">
        <ul>
            <li>
                Curve of <mark>True Positive Rate</mark> vs <mark>False Positive Rate</mark>. False Positive Rate is computed as 
                <b>1-Specificity</b>.
            </li>
            <li>
                The diagonal line is random guess.
            </li>
            <li>
                Lower than diagonal is <b>worse</b>.
            </li>
        </ul>
    </p>
  </div>
 </body>
</html>
