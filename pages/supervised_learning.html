<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="Prawar Poudel" name="author"/>
  <meta content="Vector Databases, AI, Machine Learning, NLP, GPT" name="keywords"/>
  <meta content="" name="description"/>
  <link href="../css/main.css" rel="stylesheet" type="text/css"/>
  <title>
   Supervised Learning
  </title>
 </head>
 <body>
  <!-- this one is on the top for navigation -->
  <div class="navbar">
   <a href="../index.html">
    Home
   </a>
   <a href="../about.html">
    About
   </a>
   <a href="https://www.linkedin.com/in/prawarpoudel/">
    LinkedIn
   </a>
   <a href="https://scholar.google.com/citations?user=qa8tuSIAAAAJ&amp;hl=en">
    Google Scholar
   </a>
  </div>
  <!-- following is the sidebar -->
  <div class="sidebar">
   <a class="" href="https://prawarpoudel.github.io/about">
    About
   </a>
   <a class="listt" href="../pages/ai_topics.html">
    Various AI Topics
   </a>
   <a class="listt" href="../pages/datascience_topics.html">
    Various Data Science Topics
   </a>
   <a class="listt" href="../pages/programming_topics.html">
    Various Programming Topics
   </a>
   <a class="listt" href="../pages/reading_systemdesign.html">
    System Design Topics (Book)
   </a>
   <a class="listt" href="../pages/kube_topics.html">
    Various Kubernetes Topics
   </a>
   <a class="listt" href="../pages/coding_topics.html">
    Various Coding Examples
   </a>
   <a class="listt" href="../pages/ml_basics.html">
    Machine Learning Basics
   </a>
   <a class="listt" href="../pages/rag.html">
    RAG Concept
   </a>
   <a class="listt" href="../pages/rag_code.html">
    RAG Implementation
   </a>
   <a class="listt" href="../pages/redis_benchmarking.html">
    REDIS Benchmarking
   </a>
   <a class="listt" href="../pages/codellama.html">
    Code Llama Intro
   </a>
   <a class="listt" href="../pages/codellama_handson.html">
    Code Llama Hands-On
   </a>
   <a class="listt" href="../pages/breaking_codellama.html">
    Breaking Code Llama
   </a>
   <a class="listt" href="../pages/whyjohnnycantprompt.html">
    Why Jhonny Cant't Prompt
   </a>
   <a class="listt" href="../pages/discriminative.html">
    Discriminative Vs Generative Models
   </a>
   <a class="listt" href="../pages/gan.html">
    Generative Adversarial Models
   </a>
   <a class="listt" href="../pages/regex_py.html">
    Regex in python
   </a>
   <a class="listt" href="../pages/kube_health_checks.html">
    Kube Health Checks
   </a>
   <a class="listt" href="../pages/raft_consensus.html">
    Raft Algorithm (Consensus)
   </a>
   <a class="listt" href="../pages/alerting.html">
    Ex-Google SRE on Alerting
   </a>
   <a class="listt" href="../pages/primer_systemdesign.html">
    System Design Primer (Github)
   </a>
  </div>
  <div class="content">
    <h1>
        Supervised Learning
    </h1>
    <p>
        Data points have known outcome.
    </p>
    <p>
        Goal is to predct the nature of relationship between input parameters and target variables.
    </p>
    <p>
        <b>Parameters:</b> of a machine learning model are 1 or more variables that changes their values as the model learns. No of parameters can 
        range from very few to trillions of parameters.
    </p>
    <p>
        <b>Hyperparameters:</b> are parameters that not learned directly from the data but relates to implementation.
    </p>
    <p>
        Two types of problems:
        <ul>
            <li>
                <b>Regression:</b> y or the outcome variable is numeric.
                <ul>
                    <li>
                        Outcome is continuous.
                    </li>
                </ul>
            </li>
            <li>
                <b>Classification:</b> y or outcome variable is categorical.
                <ul>
                    <li>
                        Outcome is categorical.
                    </li>
                </ul>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        Some terms of interests are:
        <ul>
            <li>
                x: input features
            </li>
            <li>
                yp: Output or the predicted values
            </li>
            <li>
                f(.): prediction function that generates predictions from x and parameters.
            </li>
            <li>
                J(y,yp): Loss function
            </li>
            <li>
                update rule: using features x and outcome y, choose parameters to minimize loss function J
            </li>
        </ul>
    </p>
    <hr>
    <p>
        Interpretation vs Prediction Objective:
        <ul>
            <li>
                <b>Interpretation:</b> Train model to find insights into data. Focus will be on parameters of the model to gain insights; and a 
                less complex models are chosen.
            </li>
            <li>
                <b>Prediction:</b> focus will be on performance metrics of the model. The model cares only about coming with best prediction so can be a black box; 
                complex models can be used. performance metrics involve closeness between yp and y i.e. predicted result vs actual result.
            </li>
        </ul>
    </p>
    <hr>
    <h2>
        Linear Regression
    </h2>
    <p>
        Measures of errors are: They can be used for any regression model
        <ul>
            <li>
                Mean Squared Error (MSE)
            </li>
            <li>
                Sum of Squared Error (SSE)  = sum (error^2)
            </li>
            <li>
                Total Sum of Squares (TSS) = Variance of error
            </li>
            <li>
                Coeffecient of Determination (R^2): 1-(SSE/TSE). Closer to 1 is better.
            </li>
        </ul>
    </p>
    <hr>
    <p>
        It is not a requirement that the target variable is normally distributed; but normally distributed target variable gives better result. <mark>What is required is 
            that the error needs to be normally distributed.
        </mark>
    </p>
    <p>
        If the target variable is not normally distributed, you can make it by transforming it. Then, fit our regression to the transformed values.
    </p>
    <p>
        To see if the target variable is normally distributed, we can see manually or do a statistical test.
        <ul>
            <li>
                For manual approach, we can plot the distribution using <mark>df['target_variable'].hist()</mark>. It should visually show the data 
                thus helping us see if the data is normally distributed.
            </li>
            <li>
                Using <mark>D'Agostino K^2 Test</mark>, you can use library function <mark>normaltest()</mark> from library <mark>scipy.stats.mstats</mark>. This gives out a p-value. 
                The higher p-value indicates the distribution is more closer to being normal. A lower p-value indicates distribution is far low probability of being a normal distribution.
                A threshold of 0.05 or 0.01 can be used for cutoff. 
            </li>
        </ul>
    </p>
    <p>
        To transform a variable (target variable) to make it normally distributed, commonly used techniques are:
        <ul>
            <li>
                <u>Log transform:</u> Just take log of the data. The data will look a lot more normal distributed. This works best for data that exhibits exponential property.
            </li>
            <li>
                <u>Square Root:</u> Just take square root of the data.
            </li>
            <li>
                <u>Box Cox:</u> It is a parameterized transformation.
                <ul>
                    <li>
                        Box-Cox transformed value of a variable y is (y^lambda -1)/lambda.
                    </li>
                    <li>
                        This is a generalization of the square root transformation, but it allows for the root value to vary and find the best one.
                    </li>
                    <li>
                        Use <mark>boxcox from scipy.stats</mark> in code; as <mark>y_transformed = boxcox(df['y'])[0]</mark>. Function <mark>boxcox</mark> returns an array,
                        the first item is the transfored array whereas the second item the lambda that was used.
                    </li>
                </ul>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        How to use:
        <ol>
            <li>
                Import sklearn library as <mark>sklearn.linear_model.LinearRegression</mark>
            </li>
            <li>
                create an object as <mark>LR=LinearRegression()</mark>. You can also pass many other hyperparameters into the object creation.
            </li>
            <li>
                Create X df from the actual dataset by dropping the target variable column so that it is easy for further computation. Similarly,
                create Y by just grabbing the target variable as it column.
            </li>
            <li>
                Fit and transform the X data with the polynomial feature object. 
                <ul>
                    <li>
                        First create an object as <mark>pf = PolynomialFeature(degree=2,include_bias=False)</mark>, that is from <mark>sklearn.preprocessing</mark>. The <mark>include_bias</mark> is False 
                        because later on LinearRegression will take care of that part.
                    </li>
                    <li>
                        Fit and transform as <mark>X_pf = pf.fit_transform(X)</mark>. X_pf now has a lot more columns that what X had.
                    </li>
                </ul>
            </li>
            <li>
                Test-Train split:
                <ul>
                    <li>
                        <mark>x_train, x_test, y_train, y_test = test_train_split(X_pf,y,test_size=0.3,random_state="some int value")</mark> where test_train_split is from 
                        <mark>sklearn.model_selection</mark>.
                    </li>
                </ul>
            </li>
            <li>
                Now apply standard scaler to the train data.
                <ul>
                    <li>
                        s = StandardScalar()
                    </li>
                    <li>
                        x_train_s = s.fit_transform(x_train)
                    </li>
                </ul>
            </li>
            <li>
                Now to bring the target variable to the normal distribution, we will use boxcox as discussed above.
                <ul>
                    <li>
                        <mark>y_train_bc = boxcox(y_train)[0]</mark>
                    </li>
                    <li>
                        We also need the lambda value for later when we need to compute inverse. so, <mark>lam=boxcox(y_train)[1]</mark>
                    </li>
                </ul>
            </li>
            <li>
                Fit the train data as <mark>LR = LR.fit(x_train_s, y_train_bc)</mark>. Here we used:
                <ul>
                    <li>
                        standard scalar fit and transfored x- data.
                    </li>
                    <li>
                        boxcoxed y- data
                    </li>
                </ul>
            </li>
            <li>
                Since the x- data used for modeling was transformed data, let us fit and transform the x_test data to StandardScalar, i.e. <mark>x_test_s = s.transform(x_test)</mark>
            </li>
            <li>
                The predicted value will be boxcoxed transformed; i.e. <mark>y_pred_bc = lr.predict(x_test_s)</mark>.
            </li>
            <li>
                To find the y_pred_bc back to the same scale as y, we can inverse boxcox. <mark>y_pred = inv_boxcox(y_pred_bc,lam)</mark>.
            </li>
            <li>
                Finally, compute the R2 score using <mark>R2 = r2_score(y_pred,y_test)</mark>.
            </li>
            <li>
                Using boxcox on the target variable improves the R2 score (higher is better). In the above example, if done without boxcox, 
                the R2 score will be lower.
            </li>
        </ol>
    </p>
    <h2>
        Data Splits and Cross Validation
    </h2>
    <p>
        To split to have a hold out data, so that its used for cross validation.
    </p>
    <p>
        Training data: 
        <ul>
            <li>
                Used for training.
            </li>
            <li>
                <mark>model(x_train,y_train).fit()</mark> = model
            </li>
        </ul>
    </p>
    <p>
        Test data:
        <ul>
            <li>
                Used for testing and prediction.
            </li>
            <li>
                <mark>model.predict(x_test)</mark> = y_pred
            </li>
            <l>
                Use actual y_test to compare and calculate <mark>test_error</mark>.
            </li>
        </ul>
    </p>
    <p>
        Syntax for test train split
        <ul>
            <li>
                <mark>from sklearn.model_selection import test_train_split</mark>
            </li>
            <li>
                <mark>train,test = test_train_split(data,test_size=0.3)</mark>
            </li>
            <li>
                <mark>x_train,x_test, y_train, y_test = test_train_split(x,y,test_size=0.3)</mark>
            </li>
            <li>
                There are other many ways for splitting, like shuffle split or stratified shuffle split. <br>
                <mark>from sklearn.model_selection import ShuffleSplit</mark>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        Categorical Data is one-hot encoded. The number of columns that is made as a result of one-hot encoding is equal to no of category values 
        - 1. The process is outline below:
        <ul>
            <li>
                Find the columns whose dtypes is np.object, i.e. <mark>mask = df.dtypes==np.object</mark>.
            </li>
            <li>
                And, filter out the columns <mark>cols = df.columns[mask]</mark>. These are the columns that we want to apply one-hot encoding to.
            </li>
            <li>
                We would also like to see if the no of unique values in the columns are more than one. If there is only 1 unique value, it does not
                make sense to one-hot encode.
            </li>
        </ul>
    </p>
    <p>
        Using <mark>sklearn.preprocessing.OneHotEncoder</mark>, instead of <mark>pd.get_dummies</mark>
        <ul>
            <li>
                <mark>df_copy = df.copy()</mark>
            </li>
            <li>
                instantiate one hot encoder object, <mark>ohc = OneHotEncoder()</mark>
            </li>
            <li>
                inside a for loop for each column, <b>col</b> in the <mark>cols</mark> list:
                <ul>
                    <li>
                        <mark>dat = ohc.fit_transform(df_copy[col])</mark>
                    </li>
                    <li>
                        drop the original column, <mark>df_copy.drop(col,axis=1)</mark>
                    </li>
                    <li>
                        get the name of all the new columns, <mark>new_cols = ohc.categories_</mark>
                    </li>
                    <li>
                        create name of columns so that its easy to join in the df later <br>
                        <mark>new_cols = ['_'.join([col,catt]) for cat in new_cols[0]]</mark>
                    </li>
                    <li>
                        create a df from the one hot encoded data <br>
                        <mark>new_df =  pd.DataFrame(dat.toarray(),columns=new_cols)</mark>
                    </li>
                    <li>
                        now append to our copy df <br>
                        <mark>df_copy = pd.concat([df_copy,new_df],axis=1 )</mark>
                    </li>
                </ul>
            </li>
            <li>
                One potential issue with too many one-hot encoded df is that Too many parameters, overfitting of model. Error will be high in test data is overfit.
            </li>
        </ul>
    </p>
    <hr>
    <p>
        Applying scalar like <mark>StandardScalar</mark> or <mark>MinMaxScalar</mark>, make sure to call <mark>fit_transform()</mark> on the training data;
        but only <mark>transform()</mark> on the test data. This should be done before applying linearregression. An example is given below.
        <ul>
            <li>
                create an object, for example <br>
                <mark>s = StandardScalar()</mark>
            </li>
            <li>
                fit transform x_train data <br>
                x_train_s = s.fit_transform(x_train)
            </li>
            <li>
                transform x_test data <br>
                x_test_s = s.transform(x_test)
            </li>
            <li>
                fit the linearRegression model <br>
                LR.fit(x_train_s,y_train)
            </li>
            <li>
                predict <br>
                predictions = LR.predict(x_test_s)
            </li>
            <li>
                find error <br>
                error = mean_squared_error(y_test,predictions)
            </li>
        </ul>
    </p>
    <h3>
        Cross Validation
    </h3>
    <p>
        Here we use multitple validation sets. These are test sets that are disjoint of each other. For each case of validation sets, the train dataset 
        can be a subset of the remaining dataset. There should be no overlap between the validation or test splits in each iteration of experiment. Training data 
        can have overlap.
    </p>
    <p>
        The average error across all these validation sets is the <mark>cross validation result</mark>.
    </p>
    <p>
        As the model gets more complex, the train error will minimize. But with the cross validation, there is an inflection point, and as the complexity
        increases, the error will increase. This is because too complex model will not generalize properly, and overfit. This we should stop increasing the complexity
        as soon as the crossvalidation error starts to increase.
        <img src="../images/crossvalidation.png" class="center-image">
    </p>
    <p>
        Coding example:
        <ul>
            <li>
                import library <br>
                <mark>from sklearn.model_selection import cross_val_score</mark>
            </li>
            <li>
                perform cross val score <br>
                <mark>cross_val = cross_val_score(model, x_data, y_data, cv=4, scoring='neg_mean_squared_error')</mark>
            </li>
            <li>
                <b>Other methods as follows are also available</b>
                <mark>from sklearn.model_selection import KFold, StratifiedKFold </mark>
            </li>
        </ul>
    </p>
    <h3>
        Using Pipeline and Crossvalidation
    </h3>
    <p>
        <ul>
            <li>
                Import all needed library
                <mark>from sklearn.linear_model import LinearRegression, Lasso, Ridge</mark>
            </li>
            <li>
                From the dataset, acquire the target variable as Y, and drop the value from original df to save it as X.
            </li>
            <li>
                Use kfold that is imported as <mark><br>from sklearn.model_selection import KFold, cross_val_predict</mark>
                <mark>
                    <br>kf = KFold(shuffle=True, random_state=6644, n_splits=3)
                </mark>
                <br>This means we will have 3 training and 3 test sets. Training sets may overlap, but test sets will not overlap.
            </li>
            <li>
                kf.slit(X) will give a generator object that has indices. For example, iterating through the generator gives a tuple with x-indices and y-indices as follows:
                <mark>
                    <br>for train_index, test_index in kf.split(X):
                </mark>
                <br>The index values, <mark>train_index, test_index</mark> can be anything from 0 to length of X-1, with length being determined by the split value.
            </li>
            <li>
                Once the indices are obtained, extract the value from the X and Y using the indices.
                <mark>
                    <br>for train_index, test_index in kf.split(X): <br>
                        ..X_train,X_test, y_train, y_test = (X.iloc[train_index], X.iloc[test_index],y[train_index],y[test_index]) <br>
                        ..lr.fit(X_train, y_train) <br>
                        ..y_pred = lr.predict(X_test) <br>
                        ..score = r2_score(y_test.values, y_pred) <br>
                </mark>
                <br> This will show the scores for all the splits.
            </li>
            <li>
                Let us add standard scalar into this. <b>Without any regularization, scaling does not help Linear Regression.</b> But just to see an example, following can be done.
                <mark>
                    <br>s = StandardScalar()
                    <br>for train_index, test_index in kf.split(X): <br>
                        ..X_train,X_test, y_train, y_test = (X.iloc[train_index], X.iloc[test_index],y[train_index],y[test_index]) <br>
                        ..X_train_s = s.fit_transform(X_train) <br>
                        ..lr.fit(X_train_s, y_train) <br>
                        ..X_test_s = s.transform(X_test)
                        ..y_pred = lr.predict(X_test_s) <br>
                        ..score = r2_score(y_test.values, y_pred) <br>
                </mark>
                <br> This will show the scores for all the splits.
            </li>
        </ul>
    </p>
    <hr>
    <p>
        Using pipeline
        <ul>
            <li>
                Sklearn allows to chain multiple operator items, as long as as they have fit() method.
            </li>
            <li>
                Here, output of one is input of another. So they also need to have fit_transform() method.
            </li>
            <li>
                The above code will now be following using pipeline:
                <ul>
                    <li>
                        <mark>s = StandardScalar()</mark> <br>
                        <mark>lr = LinearRegression()</mark>
                    </li>
                    <li>
                        <mark>my_pipe = Pipeline([('scalar',s),('regression',lr)])</mark>
                    </li>
                    <li>
                        <mark>kf = KFold(shuffle=True, random_state=6644, n_splits=3)</mark>
                    </li>
                    <li>
                        <mark>predictions = cross_val_predict(my_pipe,X,y,cv=kf)</mark>
                    </li>
                    <li>
                        <mark>r2_score(y,predictions</mark>
                    </li>
                </ul>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        <u>HyperParameter Tuning, Lasso Regression, PolynomialFeature</u>
        <ul>
            <li>
                function used is <mark>np.geomspace</mark>
            </li>
            <li>
                For <mark>Lasso Regression</mark>, higher value of alpha means the model is less complex whereas lower value of alpha means model is more complex.
            </li>
            <li>
                Less value of alpha for lasso makes the model similar to LinearRegression.
            </li>
            <li>
                Lasso is iniitalized as <br>
                <mark>lasso=Lasso(alpha=alpha, max_iter=10000)</mark>
            </li>
            <li>
                Complete usage with different values of alpha for hyperparameters selection is as follows: <br>
                <mark>
                    alphas = np.geomspace(1e-9,1e10,num=10) #creates equally spaced 10 numbers, geometrically spaced <br>
                    for alpha in alphas: <br>
                    .. lasso = Lasso(alpha=alpha,max_iter=100000) <br>
                    .. my_pipe = Pipeline([('scalar':s),("lasso":lasso)]) <br>
                    .. pred = cross_val_predict(my_pipe,X,y,cv=kf) <br>
                    .. score = r2_score(y,pred)
                </mark>
            </li>
            <li>
                With Lasso, its is always better to scale the data before using lasso regression.
            </li>
        </ul>
    </p>
    <p>
        You can add <mark>PolynomialFeature</mark> to the pipeline. What a PolynomialFeature does is very well explained 
        at <a href="https://machinelearningmastery.com/polynomial-features-transforms-for-machine-learning/">this link</a>. In a nutshell, it raises the input 
        variables to a polynomial degree. So, after applying the PolynomialFeature, the input dimension now increases.
    </p>
    <p>The above example thus becomes:
        <ul>
            <li>
                <mark>
                    pf = PolynomialFeature(degree=3)<br>
                    alphas = np.geomspace(1e-9,1e10,num=10) #creates equally spaced 10 numbers, geometrically spaced <br>
                    for alpha in alphas: <br>
                    .. lasso = Lasso(alpha=alpha,max_iter=100000) <br>
                    .. my_pipe = Pipeline([('polyfeat':pf),('scalar':s),("lasso":lasso)]) <br>
                    .. pred = cross_val_predict(my_pipe,X,y,cv=kf) <br>
                    .. score = r2_score(y,pred)
                </mark>
            </li>
        </ul>
    </p>
    <p>
        After going through the different scores based on various <mark>alpha</mark> values, we can find the best among all. And finally train the model as follows:
        <ul>
            <li>
                <mark>
                    best_pipe = Pipeline([ <br>
                        ("poly_feat":PolynomialFeature(degree=2)), <br>
                        ("scalar":s), <br>
                        ("lasso":Lasso(alpha=0.01,max_iter=100000)) <br>
                    ]) <br>
                    best_pipe.fit(X,y) <br>
                    best_pipe.score(X,y) <br>
                </mark>
            </li>
            <li>
                To see the coeffecients, we can <br>
                <mark>
                    best_pipe.named_steps['Lasso'].coef_
                </mark>
            </li>
            <li>
                Ridge Regression also works the same way as far as the coding aspect is concerned.
            </li>
            <li>
                <p class="important">
                    From the pipeline estimator that we created, we can actually see the interaction of the different input variables and the their contribution to the output variable.
                </p>
                <p>
                    From the pipeline, polynomial feature gives the higher power of the input variable as well as the interaction components. The corresponding coefficient of the Lasso/Ridge
                    regression model will indicate their relative contribution. Higher positive value indicates positive impact whereas higher negative value indicates negative impact.
                </p>
                <p>
                    <mark>from the best_pipe.named_steps['poly_feat'].get_feature_names(input_features=X.columns)</mark> gives the feature names whereas as seen before <mark>.coef_</mark> gives the lasso/ridge coefficient.
                </p>
            </li>
        </ul>
    </p>
    <hr>
    <p>
        <u>Grid Search CV</u>
    </p>
  </div>
 </body>
</html>
