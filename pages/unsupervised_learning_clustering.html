<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="Prawar Poudel" name="author"/>
  <meta content="Vector Databases, AI, Machine Learning, NLP, GPT" name="keywords"/>
  <meta content="" name="description"/>
  <link href="../css/main.css" rel="stylesheet" type="text/css"/>
  <title>
   Unsupervised Learning
  </title>
 </head>
 <body>
  <!-- this one is on the top for navigation -->
  <div class="navbar">
   <a href="../index.html">
    Home
   </a>
   <a href="../about.html">
    About
   </a>
   <a href="https://www.linkedin.com/in/prawarpoudel/">
    LinkedIn
   </a>
   <a href="https://scholar.google.com/citations?user=qa8tuSIAAAAJ&amp;hl=en">
    Google Scholar
   </a>
  </div>
  <!-- following is the sidebar -->
  <div class="sidebar">
   <a class="" href="https://prawarpoudel.github.io/about">
    About
   </a>
   <a class="listt" href="../pages/ai_topics.html">
    Various AI Topics
   </a>
   <a class="listt" href="../pages/datascience_topics.html">
    Various Data Science Topics
   </a>
   <a class="listt" href="../pages/programming_topics.html">
    Various Programming Topics
   </a>
   <a class="listt" href="../pages/reading_systemdesign.html">
    System Design Topics (Book)
   </a>
   <a class="listt" href="../pages/kube_topics.html">
    Various Kubernetes Topics
   </a>
   <a class="listt" href="../pages/coding_topics.html">
    Various Coding Examples
   </a>
   <a class="listt" href="../pages/ml_basics.html">
    Machine Learning Basics
   </a>
   <a class="listt" href="../pages/rag.html">
    RAG Concept
   </a>
   <a class="listt" href="../pages/rag_code.html">
    RAG Implementation
   </a>
   <a class="listt" href="../pages/redis_benchmarking.html">
    REDIS Benchmarking
   </a>
   <a class="listt" href="../pages/codellama.html">
    Code Llama Intro
   </a>
   <a class="listt" href="../pages/codellama_handson.html">
    Code Llama Hands-On
   </a>
   <a class="listt" href="../pages/breaking_codellama.html">
    Breaking Code Llama
   </a>
   <a class="listt" href="../pages/whyjohnnycantprompt.html">
    Why Jhonny Cant't Prompt
   </a>
   <a class="listt" href="../pages/discriminative.html">
    Discriminative Vs Generative Models
   </a>
   <a class="listt" href="../pages/gan.html">
    Generative Adversarial Models
   </a>
   <a class="listt" href="../pages/regex_py.html">
    Regex in python
   </a>
   <a class="listt" href="../pages/kube_health_checks.html">
    Kube Health Checks
   </a>
   <a class="listt" href="../pages/raft_consensus.html">
    Raft Algorithm (Consensus)
   </a>
   <a class="listt" href="../pages/alerting.html">
    Ex-Google SRE on Alerting
   </a>
   <a class="listt" href="../pages/primer_systemdesign.html">
    System Design Primer (Github)
   </a>
   <a class="listt" href="../pages/supervised_learning.html">
    Supervised Machine Learning
   </a>
   <a class="listt" href="../pages/unsupervised_learning_clustering.html">
    Unsupervised Machine Learning (Clustering)
   </a>
   <a class="listt" href="../pages/unsupervised_learning_dimsreduction.html">
    Unsupervised Machine Learning (Dimensn Redcsn)
   </a>
   <a class="listt" href="../pages/unsupervised_learning_timeseries.html">
    Unsupervised Machine Learning (Timeseries)
   </a>
   <a class="listt" href="../pages/deep_learning.html">
    Deep Learning (Introduction)
   </a>
  </div>
  <div class="content">
   <h1>
    Unsupervised Learning
   </h1>
   <p>
    <ul>
     <li>
      Datapoints do not have any outcomes, or target is unknown.
     </li>
     <li>
      We are interested in the structure of the data or the patterns within the data.
     </li>
     <li>
      Types:
      <ul>
       <li>
        <a href="unsupervised_learning_clustering.html">
         <b>
          Clustering:
         </b>
        </a>
        Algorithm like:
        <ul>
         <li>
          K-Means
         </li>
         <li>
          Hierarchical Agglomerative Clustering
         </li>
         <li>
          DBSCAN
         </li>
         <li>
          Mean shift
         </li>
        </ul>
       </li>
       <li>
        <a href="unsupervised_learning_dimsreduction.html">
         <b>
          Dimensionality Reduction:
         </b>
        </a>
        Algorithm like:
        <ul>
         <li>
          PCA
         </li>
         <li>
          Non-negative matrix factorization
         </li>
         <li>
          They are important because of the
          <mark>
           curse of dimensionality
          </mark>
          .
         </li>
         <li>
          .. which means that as no of features increases performance gets worse, and cost or the number of training examples required increases.
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li>
      Many use cases like:
      <ul>
       <li>
        Classification
       </li>
       <li>
        Anomaly Detection
       </li>
       <li>
        Customer Segmentation
       </li>
       <li>
        Improve Supervised Learning
       </li>
      </ul>
     </li>
    </ul>
   </p>
   <hr/>
   <h2>
    Clustering
   </h2>
   <h3>
    K-Means Clustering
   </h3>
   <p>
    <ul>
     <li>
      Following is the algorithm:
      <ol>
       <li>
        Take K random points. These are our centroids
       </li>
       <li>
        For each point in the dataset, compute the distance of the point with each centroids.
       </li>
       <li>
        Each point falls in the cluster of the nearest centroid.
       </li>
       <li>
        Now, adjust the centroids. i.e. for each clusters, find new centroids that are mean of points in each clusters.
       </li>
       <li>
        Repeat Steps 2, 3 and 4 above until no new points are assigned to a new cluster.
       </li>
      </ol>
     </li>
     <li>
      Smarter initialization of the centroids:
      <ul>
       <li>
        Pick first centroid at random.
       </li>
       <li>
        <mark>
         k-Means ++
        </mark>
        : For the next centroid, pick the farthest distance using the probability
        <mark>
         distance^2/sum(distance^2)
        </mark>
        .  i.e. for each point 
                        we compute the above metric where distance is from the first centroid. Pick the one with max probability. TODO how 
                        other centroids are picked.
       </li>
      </ul>
     </li>
     <li>
      Picking the best cluster:
      <ul>
       <li>
        <b>
         Inertia
        </b>
        : sum of squared distance from each point to its cluster.
        <mark>
         sum(x-c)^2
        </mark>
        , where x is data points in the cluster and c is the centroid.
        <ul>
         <li>
          smaller value means tighter cluster.
         </li>
         <li>
          but, value is sensitive to no of points in the cluster.
         </li>
        </ul>
       </li>
       <li>
        <b>
         Distortion
        </b>
        : average of sum of distance, which is essentially
        <mark>
         Interial/no of data points in cluster
        </mark>
        .
        <ul>
         <li>
          This would still mean smaller value mean tigher cluster.
         </li>
         <li>
          does not increase as moere points are added
         </li>
        </ul>
       </li>
       <li>
        Of the different models with different values of k, pick the one with best (low)
        <mark>
         inertia
        </mark>
        or
        <mark>
         Distortion
        </mark>
        .
       </li>
      </ul>
     </li>
     <li>
      Interia or Distortion can be used to find the right no of clusters also.
      <ul>
       <li>
        Inertia and Distortion both decrease with increae in K. (lower is better)
       </li>
       <li>
        But in the plot of Inertia/Distortion vs K, there is an elbow or inflection point at which the rate of decrease in inertia or distortion is much less.
       </li>
       <li>
        When there is no apparant elbow, there is a method called
        <mark>
         Silhouette method
        </mark>
        to help determine the optimal value of K.
       </li>
      </ul>
     </li>
    </ul>
   </p>
   <p class="code-text">
    from sklearn.cluster import KMeans

km = KMeans(n_clusters=3,init='k-means++')
km = km.fit(X1)
y_predict = km.predict(X2)
print(km.interia_)
   </p>
   <p>
    <ul>
     <li>
      Batch mode for k-means can be used by
      <mark>
       MiniBatchKMeans
      </mark>
      , that works with random batches of data
     </li>
    </ul>
   </p>
   <hr/>
   <h2>
    Other Clustering Algorithms
   </h2>
   <p>
    <ul>
     <u>
      <b>
       Distance Metrics:
      </b>
     </u>
     <li>
      Euclidean Distance:
      <ul>
       <li>
        The usual geometric distance metric
       </li>
       <li>
        Also called L2 distance, because we take 2nd power and sum them and take square root.
       </li>
       <li>
        this is more sensitive to curse of dimensionality, compared to cosine distance
       </li>
      </ul>
     </li>
     <li>
      Manhattan Distance:
      <ul>
       <li>
        Take difference between the two points in each axis, and add the absolute values.
       </li>
       <li>
        Also called L1 distance. Unlike L2, where we take 2nd power and 2nd root after sum, this can be thought of as taking n=1 instead of  n=1.
       </li>
       <li>
        L1 is always larger than L2, unless they lie on same axis, in which case, they will be same.
       </li>
       <li>
        for high number of dimensions, we take L1 distance because it tends to perform better than L2 distance.
       </li>
      </ul>
     </li>
     <li>
      Cosine Distance
      <ul>
       <li>
        Take a cosine of the angle between the vectors defined by the two points (the two points between which we need to compare the distance.)
       </li>
       <li>
        The distance represents the angle cosine values of vectors  to the origin from the points.
       </li>
       <li>
        Thus replacing a point with another distant or nearby points, but falling on the same vector as original point to the origin, the cosine distance remains unchanged.
       </li>
       <li>
        Cosine distance is better suited for text data, where location of occurrence is less important.
       </li>
      </ul>
     </li>
     <li>
      Jaccard Distance
      <ul>
       <li>
        Applies to set, for example word occurrence.
       </li>
       <li>
        computed as 1-(intersection of two sets)/(union of two sets)
       </li>
      </ul>
     </li>
    </ul>
   </p>
   <h3>
    Hierarchical Agglomerative Clustering
   </h3>
   <p>
    <ul>
     <li>
      Very useful in businesses, when we are interested in what subgroups make up the clusters
     </li>
     <li>
      Algorithm:
      <ul>
       <li>
        Find the closest pair, and merge them into a cluster.
       </li>
       <li>
        Find next closest pair, and merge them into a cluster.
       </li>
       <li>
        The next pair could be another set of points, or cluster. In case of clusters, merge the clusters into a single cluster.
       </li>
       <li>
        The distance between a cluster and a point or another cluster is defined by Linkage Criteria. Most common use cases are taking average of all points in the cluster, or 
                        for a point and a cluster, find the minumum distance from the point to the cluster, etc
       </li>
       <li>
        Stop when the number of cluster is the required number of clusters. OR, finding the average cluster distance with a predefined
                        threshold. For the average cluster distance concept, the Agglomeration does not stop until the minumum of the average cluster distance is not 
                        above the threshold.
       </li>
      </ul>
     </li>
     <li>
      Linkage Criteria:
      <ul>
       <li>
        <b>
         Single Linkage:
        </b>
        Miminum pairwise distance between clusters. i.e. for two clusters, it is the distance between the closest points in those clusters. In other words, the minimum
                        possible distance between the clusters.
        <b>
         Pros
        </b>
        : Clear boundary definition;
        <b>
         Con
        </b>
        : Affected by outlier easily, and cannot separate cleanly.
       </li>
       <li>
        <b>
         Complete Linkage:
        </b>
        Take the maximum distance possible between the points in two clusters. From among those, take the min distance to find which cluster to merge.
        <b>
         Pro
        </b>
        : Works well with noise or a slight overlap.
        <b>
         Con
        </b>
        : Tend to break apart a large cluster depending upon where the max point lies.
       </li>
       <li>
        <b>
         Average Linkage:
        </b>
        Take average distance. But it can inherit cons of both above methods, while also inheriting pros of the above two methods.
       </li>
       <li>
        <b>
         Ward Linkage:
        </b>
        Merges the cluster that tends to minimize the inertia. Pros and cons are again similar to average linkage.
       </li>
      </ul>
     </li>
    </ul>
   </p>
   <p class="code-text">
    import sklearn.cluster import AgglomerativeClustering

agc = AgglomerativeClustering(n_clusters=3, affinity='euclidean',
                            linkage='ward')

agc = agc.fit(X)                        
y_predict = agc.predict(Xx)
   </p>
   <h3>
    DBSCAN
   </h3>
   <p>
    <ul>
     <li>
      it is true clustering algorithm; there can be points that do not belong to any cluster.
     </li>
     <li>
      can handle noise better
     </li>
     <li>
      Basic idea is that points in a cluster should belong within certain distance, in a neighborhood
     </li>
     <li>
      find core point in high density regions, and expand them with more points.
     </li>
     <li>
      Algorithm ends when no more points remain within certain distance of the cluster centroids.
     </li>
    </ul>
    <ul>
     Algorithm
     <li>
      Inputs required:
      <ul>
       <li>
        Metric: distance metric
       </li>
       <li>
        Epsilon (eps): radius of local neighborhood
       </li>
       <li>
        n_clu: if a point has n_clu points in its neighborhood including itself, it can be considered a core point and form its own cluster
       </li>
      </ul>
     </li>
     <li>
      Any point could have three labels:
      <ul>
       <li>
        Core: its a core point
       </li>
       <li>
        density-reachable: its within the eps neighborhood of a core point,  but itself does not qualify for a core point due to less n_clu
       </li>
       <li>
        noise: a point that has no-core points in its neighborhood.
       </li>
      </ul>
     </li>
    </ul>
    <ul>
     Pros:
     <li>
      can find the optimal number of clusters itself
     </li>
     <li>
      works very well with irregular shaped cluster
     </li>
     <li>
      can handle noise very well
     </li>
    </ul>
    <ul>
     Cons:
     <li>
      requires two parameters, vs other that require one
     </li>
     <li>
      two hyperparameter tuning could be difficult
     </li>
     <li>
      does not do well with clusters with different density. (single distance metric may not capture different densities)
     </li>
    </ul>
   </p>
   <p class="code-text">
    from sklearn.cluster import DBSCAN

db = DBSCAN(eps=3, min_samples=2)
db.fit(X)

# clusters
clusters = db.labels_
   </p>
   <p>
    There is no predict because of the nature of the algorithm. If we want new data, just call fit again and see labels for cluster assignment.
    <mark>
     Outliers are assigned -1.
    </mark>
   </p>
   <h3>
    Mean Shift
   </h3>
   <p>
    <ul>
     <li>
      similar to k-means, assigns point to nearest cluster density.
     </li>
     <li>
      centroid is going to be the most dense point in the cluster, could be anywhere in the cluster.
     </li>
     <li>
      density is computed as weighted mean around each point
     </li>
     <li>
      <ol>
       Algorithm:
       <li>
        Choose a point and a window W
       </li>
       <li>
        Find the weighted mean in W.
       </li>
       <li>
        Shift the centroid of the window to the new mean
       </li>
       <li>
        Repeat 2 and 3 above until no local density maximum ("mode") is reached
       </li>
       <li>
        Repeat 1-4 for each data points
       </li>
       <li>
        Data points that lead to the same mode are grouped together into the same cluster
       </li>
      </ol>
     </li>
     <li>
      weighted Mean Computed as:
      <br/>
      <img class="center-image" src="../images/meanshift.png"/>
     </li>
    </ul>
    <ul>
     Pros:
     <li>
      model free; no assumption of no or shapes in clusters; can form uneven cluster size, but does not handle weird shapes cluster well
     </li>
     <li>
      only 1 parameters; i.e. window size
     </li>
     <li>
      robust to outliers
     </li>
    </ul>
    <ul>
     Cons:
     <li>
      result depend on window size
     </li>
     <li>
      window selection may be hard
     </li>
     <li>
      computationally expensive, mn^2, where m=iterations, n=no of points
     </li>
    </ul>
   </p>
   <p clas="code-text">
    from sklearn.cluster import MeanShift

ms = MeanShift(bandwidth=3)

ms.fit(X)
ms.predict(Xx)
   </p>
   <hr/>
   <p>
    <a href="unsupervised_learning_dimsreduction.html">
     Dimensionality Reduction at separate page
    </a>
    .
   </p>
  </div>
 </body>
</html>
